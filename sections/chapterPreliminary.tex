\chapter{Preliminaries and Related Work}
\section{Asymptotic Behavior and Convergence Analysis}
\label{sec:prelim-convergence}

Throughout this work we will study quantities that depend on a size or
iteration parameter, such as the system size $N$, the number of basis
functions, ranks of low-rank approximations, or the iteration index $k$ of an
iterative scheme. Their long-term behavior as $N \to \infty$ or $k \to \infty$
will be captured using a unified language of \emph{asymptotic behavior}. In
this section we first recall basic notions such as $\mathcal{O}(N^k)$ and
related asymptotic notation, and explain how they will be used throughout the
paper. We then view convergence orders of iterative methods---in particular
the classical Q-order and the Ortega--Rheinboldt R-order---as specific
applications of this asymptotic analysis to error sequences.

\subsection{Asymptotic behavior of sequences and functions}
\label{subsec:asymptotic}

Let $\{a_n\}_{n\in\mathbb{N}}$ and $\{b_n\}_{n\in\mathbb{N}}$ be real sequences
with $b_n > 0$ for all sufficiently large $n$. We say that
\begin{itemize}
  \item $a_n = \mathcal{O}(b_n)$ as $n \to \infty$ if there exist constants
        $C > 0$ and $n_0 \in \mathbb{N}$ such that
        \[
          |a_n| \le C\,b_n \quad \forall\, n \ge n_0.
        \]
        Intuitively, $a_n$ grows no faster than $b_n$ up to a constant factor.
  \item $a_n = o(b_n)$ as $n \to \infty$ if
        \[
          \lim_{n\to\infty} \frac{|a_n|}{b_n} = 0,
        \]
        i.e., $a_n$ is asymptotically negligible compared to $b_n$.
  \item $a_n = \Theta(b_n)$ as $n\to\infty$ if there exist $c_1, c_2 > 0$ and
        $n_0 \in \mathbb{N}$ such that
        \[
          c_1 b_n \le |a_n| \le c_2 b_n \quad \forall\, n \ge n_0.
        \]
        In this case $a_n$ and $b_n$ have the same growth rate up to constant
        factors.
\end{itemize}
Analogous definitions apply to functions $a(t)$ and $b(t)$ as $t \to \infty$ or
$t \to 0^+$ by considering $t$ as a continuous parameter. We will freely switch
between sequence and function notation depending on the context.

A particularly important family of asymptotic behaviors in this work is
\emph{polynomial} and \emph{exponential} scaling with respect to a size
parameter $N \in \mathbb{N}$. We say that a quantity has
\begin{itemize}
  \item polynomial or algebraic complexity of order $k$ if it scales as
        $\mathcal{O}(N^k)$ for some $k > 0$;
  \item (sub)exponential complexity if it scales as
        $\exp(\mathcal{O}(N^k))$ or faster for some $k > 0$.
\end{itemize}
For example, the storage cost of a dense $N\times N$ matrix is
$\Theta(N^2)$, while the cost of enumerating all determinants in a full
configuration interaction (FCI) expansion grows combinatorially with $N$ and
is often described informally as ``exponential'' in the size of the active
space.

In the context of iterative methods, we are interested in the asymptotic decay
of an \emph{error sequence} $\{\xi_k\}$ as $k \to \infty$. Typical behaviors
include
\begin{itemize}
  \item geometric or exponential decay, e.g.\ $\xi_k = \mathcal{O}(\rho^k)$
        with $0 < \rho < 1$ (``linear'' convergence);
  \item polynomial decay, e.g.\ $\xi_k = \mathcal{O}(k^{-\alpha})$ with
        $\alpha > 0$ (sublinear convergence);
  \item faster-than-linear decay, such as $\xi_k = \mathcal{O}(\rho^{k^2})$ or
        $\xi_k = \mathcal{O}(\rho^k / \log k)$ (various forms of superlinear
        convergence).
\end{itemize}

Throughout the paper we will use this asymptotic language at several levels:
\begin{itemize}
  \item \emph{Convergence analysis.} Convergence rates of iterative methods
        are expressed in terms of asymptotic decay rates of error sequences
        (Sections~\ref{subsec:q-order}--\ref{subsec:r-order}).
  \item \emph{Complexity analysis.} Storage and arithmetic costs of low-rank
        and hierarchical matrix formats will be described using
        $\mathcal{O}(\cdot)$ and $\Theta(\cdot)$ notation in terms of system
        size, basis size, and ranks.
  \item \emph{Approximation error.} Truncation errors in singular value
        decompositions (SVD/TSVD), strong rank-revealing QR (SRRQR),
        hierarchical matrices, and wavefunction/tensor compressions will be
        quantified using $\mathcal{O}(\cdot)$ notation in the relevant norms.
\end{itemize}
By fixing this asymptotic viewpoint at the outset, we can treat convergence
rates, algorithmic complexity, and approximation errors within a unified
framework of asymptotic behavior.

\subsection{Q-order of convergence}
\label{subsec:q-order}

We now recall how asymptotic behavior can be used to describe the convergence
of iterative methods. Consider a generic fixed-point iteration
\begin{equation}
  x_{k+1} = g(x_k), \qquad k = 0,1,2,\dots,
\end{equation}
aiming to compute a fixed point $x^\ast$ satisfying $x^\ast = g(x^\ast)$, or
equivalently a root of a nonlinear system $f(x^\ast) = 0$ after writing
$g(x) = x - A(x) f(x)$. Let $e_k = x_k - x^\ast$ and
$\xi_k = \|e_k\|$. The asymptotic behavior of the sequence $\{\xi_k\}$ as
$k\to\infty$ characterizes the convergence of the method.

Following Traub \cite{Traub1964}, the sequence $\{x_k\}$ is said to converge
to $x^\ast$ with \emph{Q-order} $r \ge 1$ if there exists a constant
$C \in (0,\infty)$ such that
\begin{equation}
  \label{eq:q-order}
  \lim_{k \to \infty}
  \frac{\|x_{k+1}-x^\ast\|}{\|x_k - x^\ast\|^r}
  = C.
\end{equation}
If $r = 1$ and $0 < C < 1$, the convergence is \emph{Q-linear} with asymptotic
rate factor $C$; if $r > 1$ the convergence is \emph{Q-superlinear}, and if
$r = 2$ it is \emph{Q-quadratic}. In terms of asymptotic notation, Q-linear
convergence corresponds roughly to $\|e_k\| = \Theta(\rho^k)$ for some
$\rho \in (0,1)$, whereas higher Q-orders describe faster regimes, such as
$\Theta(\rho^{k^2})$.

Q-order is simple and has been widely used in the analysis of Newton-type
methods and related algorithms; see, e.g.,
\cite{Ostrowski1966,OrtegaRheinboldt1970,DennisSchnabel1996}. However, it has
two well-known drawbacks
\cite{Ostrowski1966,PotraPtak1984,Potra1989,Jay2002}. First, it is
\emph{norm dependent}: the rate factor $C$ in \eqref{eq:q-order} depends on the
chosen norm, which complicates comparisons between different problems and
different vector norms. Second, the definition implicitly assumes a form of
asymptotic uniformity---roughly speaking, the local error reduction must behave
like a fixed power of $\|e_k\|$ in the limit. As a result, Q-order can be
ill-suited for non-monotone or highly non-uniform error sequences, which are
common in modern iterative schemes with globalization strategies, adaptive
steps, or stochastic perturbations.

These limitations motivate more flexible notions of convergence order that are
better aligned with the asymptotic behavior viewpoint of
Section~\ref{subsec:asymptotic}.

\subsection{R-order of convergence}
\label{subsec:r-order}

To relax the asymptotic uniformity requirement and to support non-monotone
sequences, Ortega and Rheinboldt introduced the \emph{R-order} (``root order''
of convergence) \cite{OrtegaRheinboldt1970}. The idea is to compare the error
to an auxiliary scalar sequence with prescribed Q-order. A sequence
$\{x_k\}$ is said to converge to $x^\ast$ with R-order $r$ if there exists a
nonnegative scalar sequence $\{\xi_k\}$ such that
\begin{equation}
  \|x_k - x^\ast\| \le \xi_k, \qquad
  \lim_{k\to\infty}\frac{\xi_{k+1}}{\xi_k^r} = C_r \in (0,\infty),
\end{equation}
for some $r \ge 1$. In this way, $\{\xi_k\}$ provides an upper bound for the
error and has the desired Q-order-$r$ behavior. This definition is naturally
norm independent and can accommodate non-monotone error sequences.

Based on this construction, one can define \emph{R-linear}, \emph{R-superlinear},
and \emph{R-quadratic} convergence, which are widely used in nonlinear equation
solving and optimization
\cite{OrtegaRheinboldt1970,DennisSchnabel1996,NocedalWright2006,SunYuan2006}.
From the asymptotic viewpoint, R-order describes how fast the error can be
bounded above by a scalar sequence with a given exponential-type decay.

However, R-order also has important limitations that have been discussed in
detail in \cite{PotraPtak1984,Potra1989,Jay2002}. Because R-order is based on
an upper bounding sequence, it often fails to distinguish between different
sublinear rates: many qualitatively different behaviors (such as
$\mathcal{O}(k^{-1})$ versus $\mathcal{O}(k^{-1/2})$) can share the same
R-order. Moreover, the notion of ``R-order-1'' does not coincide with
``R-linear'' in general. For instance, sequences of the form
\begin{equation}
  \|x_k - x^\ast\| = 0.5^k \,\log k
  \quad\text{or}\quad
  \|x_k - x^\ast\| = \frac{0.5^k}{\log k}
\end{equation}
can both be classified as R-order-1, yet the former is superlinear and the
latter is sublinear in the conventional sense. Similar ambiguities occur for
$r>1$ when one tries to distinguish R-order-$r$ from truly $r$th-order
convergence.

Consequently, while Q-order and R-order provide a useful first level of
classification for the asymptotic behavior of error sequences, they do not
always offer the fine-grained resolution needed to compare modern iterative
methods, especially in the intermediate regimes between strictly linear and
strictly quadratic convergence. This motivates the development of more refined
convergence frameworks, which will be introduced and applied later in this
work.

\section{Linear Algebra Tools for Low-Rank and Hierarchical Approximation}
\label{sec:prelim-la}

Many of the algorithms considered in this work rely on low-rank and hierarchical
approximations of large matrices and tensors. In this section we review the
basic linear algebra tools underlying such approximations: singular value
decomposition (SVD) and truncated SVD, strong rank-revealing QR (SRRQR)
factorization, and hierarchical matrix formats including $\mathcal{H}$- and
$\mathcal{H}^2$-matrices. Throughout, we emphasize their asymptotic storage and
computational complexities in the sense of Section~\ref{subsec:asymptotic}.

\subsection{Singular value decomposition and truncated SVD}
\label{subsec:svd}

Let $A \in \mathbb{R}^{m\times n}$ with $m \ge n$. The singular value
decomposition (SVD) of $A$ is
\begin{equation}
  A = U \Sigma V^\top,
\end{equation}
where $U \in \mathbb{R}^{m\times m}$ and $V \in \mathbb{R}^{n\times n}$ are
orthogonal matrices, and $\Sigma \in \mathbb{R}^{m\times n}$ is diagonal with
nonnegative entries
\[
  \sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_n \ge 0,
\]
called the singular values of $A$. The classical Eckart--Young theorem states
that for any $1 \le k \le n$, the best rank-$k$ approximation of $A$ in either
the spectral or Frobenius norm is obtained by truncating the SVD:
\begin{equation}
  A_k := U_{(:,1:k)} \Sigma_{1:k,1:k} V_{(:,1:k)}^\top.
\end{equation}
The approximation error satisfies
\begin{equation}
  \|A - A_k\|_2 = \sigma_{k+1}, \qquad
  \|A - A_k\|_F^2 = \sum_{j>k} \sigma_j^2,
\end{equation}
see, e.g., \cite{EckartYoung1936,GolubVanLoan2013}. Thus the decay of the
singular values directly controls the asymptotic behavior of the truncation
error as a function of $k$.

If $A$ is dense with $m \ge n$, a full SVD costs
$\mathcal{O}(mn^2)$ operations and requires $\Theta(mn)$ storage, which is
prohibitive for very large problems. In many applications, however, the
effective numerical rank of $A$ is much smaller than $\min\{m,n\}$, and it
suffices to compute a low-rank approximation of rank $k \ll \min\{m,n\}$. In
this case one uses a \emph{truncated SVD} (TSVD), computed by algorithms that
target only the leading $k$ singular triplets. Dense algorithms, Krylov
subspace methods, and randomized algorithms can reduce the cost to roughly
$\mathcal{O}(mnk)$, up to logarithmic factors, while maintaining near-optimal
accuracy \cite{Hansen1998,MartinssonTropp2020}.

From the point of view of asymptotic behavior, if the singular values of $A$
decay polynomially, say $\sigma_j = \mathcal{O}(j^{-\alpha})$ for some
$\alpha > 1/2$, then the TSVD error in the Frobenius norm satisfies
\[
  \|A - A_k\|_F
  = \Theta\!\Bigl(\bigl(\sum_{j>k} j^{-2\alpha}\bigr)^{1/2}\Bigr)
  = \mathcal{O}(k^{1/2-\alpha}),
\]
while exponential decay of $\sigma_j$ leads to exponentially small truncation
errors. These relations will be used later to interpret low-rank tensor and
matrix approximations in terms of their asymptotic error behavior.

\subsection{Strong rank-revealing QR factorization}
\label{subsec:srrqr}

While SVD-based low-rank approximations are optimal in a least-squares sense,
they can be relatively expensive and may not preserve sparsity. Rank-revealing
QR (RRQR) factorizations provide a cheaper alternative that approximates the
behavior of the SVD while relying only on orthogonal transformations and column
permutations.

Given $A \in \mathbb{R}^{m\times n}$ with $m \ge n$, a column-pivoted QR
factorization has the form
\begin{equation}
  A \Pi
  = Q
  \begin{bmatrix}
    R_{11} & R_{12}\\
    0      & R_{22}
  \end{bmatrix},
\end{equation}
where $\Pi$ is a permutation matrix, $Q$ is orthogonal, and $R$ is upper
triangular. A \emph{strong rank-revealing QR} (SRRQR) factorization, as
proposed by Gu and Eisenstat \cite{GuEisenstat1996}, imposes additional bounds
on $\|R_{11}^{-1} R_{12}\|$ and $\|R_{22}\|$, ensuring that the leading columns
selected by $\Pi$ form a numerically stable basis for the dominant column
space of $A$. For a target rank $k$, SRRQR yields an approximation of the form
\begin{equation}
  A \approx A_{(:,J)} X,
\end{equation}
where $J$ indexes $k$ selected columns and $X$ is a well-conditioned
interpolation matrix.

The cost of computing an SRRQR factorization of an $m\times n$ dense matrix is
comparable to that of a standard pivoted QR factorization,
$\mathcal{O}(mn^2)$ in the worst case, but with significantly improved
rank-revealing properties. In practice, SRRQR is often applied to many
moderate-size blocks (for example, in a hierarchical matrix structure), so that
the overall complexity scales as a sum of local costs. From the asymptotic
viewpoint, SRRQR provides low-rank approximations whose error is within a
moderate factor of the optimal TSVD error, but at a lower total cost when
applied blockwise.

\subsection{Hierarchical matrices}
\label{subsec:h-matrix}

Hierarchical matrices ($\mathcal{H}$-matrices) are data-sparse representations
for large, structured matrices arising in applications such as elliptic partial
differential equations, boundary integral equations, and kernel methods
\cite{HackbuschBorm2002,Bebendorf2008,Hackbusch2015}. The main idea is to
exploit the fact that matrix blocks corresponding to well-separated index
clusters are often numerically low rank, while blocks corresponding to
near-field interactions are not.

More precisely, one first organizes the index set
$\mathcal{I} = \{1,\dots,N\}$ into a cluster tree based on geometric or
algebraic information, and then constructs a block cluster tree that partitions
the $N\times N$ matrix into blocks. An admissibility condition---typically
expressing that the underlying index clusters are well separated---is used to
classify blocks as \emph{far-field} or \emph{near-field}. Near-field blocks are
stored explicitly as dense matrices, while far-field blocks are compressed by
low-rank approximations, often obtained by SVD, RRQR, or SRRQR.

Under mild assumptions on the underlying problem and the admissibility
condition, $\mathcal{H}$-matrices achieve almost linear storage and arithmetic
complexity. For example, for a second-order elliptic operator in three
dimensions, one can typically store the corresponding stiffness matrix in
$\mathcal{O}(N \log N)$ memory and apply it to a vector in
$\mathcal{O}(N \log^2 N)$ time, where $N$ is the number of degrees of freedom
\cite{Hackbusch2015}. In the language of Section~\ref{subsec:asymptotic}, this
represents a substantial improvement over the $\Theta(N^2)$ storage and
$\Theta(N^2)$ matrix-vector multiplication cost of a dense matrix.

The hierarchical organization is particularly flexible: blocks can be further
subdivided, admissibility can be adapted to the problem at hand, and different
low-rank approximation techniques can be used in different parts of the
matrix. This flexibility makes $\mathcal{H}$-matrices a natural tool for
compressing large matrices and tensors in many-body problems.

\subsection{$\mathcal{H}^2$-matrices}
\label{subsec:h2-matrix}

While $\mathcal{H}$-matrices already reduce storage and computation costs
significantly, they store separate low-rank bases for different far-field
blocks, which leads to redundancy. $\mathcal{H}^2$-matrices refine this idea by
introducing \emph{nested} cluster bases that are shared across blocks
\cite{BormGrasedyckHackbusch2003,Hackbusch2015}. In an $\mathcal{H}^2$-matrix,
each cluster in the index tree is associated with a basis that spans the
relevant far-field interactions, and bases at coarse levels are represented in
terms of bases at finer levels via small transfer matrices.

This nested structure eliminates much of the redundancy present in standard
$\mathcal{H}$-matrices. Under suitable assumptions on the kernel or operator,
$\mathcal{H}^2$-matrices can achieve linear or nearly linear complexity:
\begin{equation}
  \mathcal{O}(N) \quad\text{for storage}, \qquad
  \mathcal{O}(N) \quad\text{for matrix-vector multiplication},
\end{equation}
again up to logarithmic factors and assuming bounded ranks. From the asymptotic
viewpoint, this represents an improvement from $\mathcal{O}(N \log N)$ or
$\mathcal{O}(N \log^2 N)$ to (essentially) $\mathcal{O}(N)$, while maintaining
a prescribed accuracy in the relevant matrix norm.

$\mathcal{H}^2$-matrices are particularly effective for discretizations of
translation-invariant or asymptotically smooth kernels, such as Green's
functions of elliptic operators and Coulomb-like interactions. In later
chapters we will make use of hierarchical and $\mathcal{H}^2$-matrix ideas to
compress large matrices and tensors arising in electronic-structure theory,
and we will analyze their storage, computational complexity, and approximation
errors using the asymptotic framework introduced in
Section~\ref{sec:prelim-convergence}.

\section{Quantum Chemistry Background}
\label{sec:prelim-chem}

We now review the quantum chemistry background relevant to this work, focusing
on the tensor structures and electronic-structure methods that will be used in
later sections. We first introduce the full configuration interaction (FCI)
tensor and the electron repulsion integral (ERI) tensor, and then briefly
review the Hartree--Fock (HF) method and second-order M{\o}ller--Plesset
perturbation theory (MP2). Throughout, we emphasize the asymptotic behavior of
the computational cost in terms of the number of basis functions and electrons.

\subsection{Full configuration interaction tensor}
\label{subsec:fci-tensor}

Consider an $N$-electron system in a finite one-particle basis of $M$
spin-orbitals $\{\chi_p\}_{p=1}^M$. Within this basis, the electronic
wave function can be expanded in a basis of Slater determinants
$\{\Phi_I\}$ constructed by occupying $N$ spin-orbitals out of $M$:
\begin{equation}
  \Psi
  = \sum_{I} C_I \Phi_I,
  \qquad
  \Phi_I = \frac{1}{\sqrt{N!}}
  \det\bigl[\chi_{i_1}(1)\,\chi_{i_2}(2)\,\dots\,\chi_{i_N}(N)\bigr],
\end{equation}
where $I = (i_1,\dots,i_N)$ indexes the occupied spin-orbitals in determinant
$\Phi_I$ and $C_I$ are configuration interaction (CI) coefficients. In full
configuration interaction (FCI), the sum runs over all determinants consistent
with the specified number of electrons and spin symmetry, leading to a formally
exact solution of the nonrelativistic electronic Schr\"odinger equation within
the chosen basis \cite{SzaboOstlund1989,HelgakerJorgensenOlsen2000}.

The dimension of the determinant basis is
\begin{equation}
  N_\mathrm{det}
  = \binom{M}{N_\alpha} \binom{M}{N_\beta},
\end{equation}
where $N_\alpha$ and $N_\beta$ are the numbers of $\alpha$- and $\beta$-spin
electrons, respectively. As $M$ grows, $N_\mathrm{det}$ increases
combinatorially, which leads to an exponential scaling of the FCI cost with
respect to system size. In the asymptotic language of
Section~\ref{subsec:asymptotic}, the storage required for the CI coefficients
is $\Theta(N_\mathrm{det})$, which behaves roughly like
$\exp(\mathcal{O}(M))$ for fixed electron fraction.

It is often convenient to view the collection of CI coefficients as a tensor,
the \emph{FCI tensor}. For example, in an occupation-number representation the
FCI tensor has one index per spin-orbital,
\begin{equation}
  C_{n_1 n_2 \dots n_M},
  \qquad n_p \in \{0,1\},
\end{equation}
subject to the constraint $\sum_p n_p = N$. Alternatively, one can group
spin-orbitals into $\alpha$- and $\beta$-strings and fold the CI vector into a
matrix with indices $(I_\alpha, I_\beta)$, where each index labels an
occupation pattern of $\alpha$- or $\beta$-spin orbitals. Regardless of the
specific indexing, the resulting FCI tensor is extremely high-dimensional and
dense, and serves as a prototypical example of an object with exponential
storage requirements.

Because of this unfavorable asymptotic behavior, practical electronic-structure
calculations rely on truncated CI expansions (CISD, CISDTQ, etc.), coupled-cluster
theory, selected CI, density-matrix renormalization group (DMRG), tensor
network states, and various low-rank or sparsity-exploiting wave function
approximations; see, e.g., \cite{SzaboOstlund1989,HelgakerJorgensenOlsen2000,
SherrillSchaefer1999,ChanHeadGordon2002,Schollwock2011} for reviews. In later
sections we will revisit the FCI tensor as a motivating example for hierarchical
and low-rank tensor approximations.

\subsection{Electron repulsion integral tensor}
\label{subsec:eri-tensor}

Let $\{\varphi_\mu\}_{\mu=1}^{N_\mathrm{bas}}$ denote a set of spatial basis
functions (typically Gaussian-type orbitals). The two-electron Coulomb
interaction in this basis is encoded by the electron repulsion integral (ERI)
tensor
\begin{equation}
  (\mu\nu|\lambda\sigma)
  =
  \iint
  \varphi_\mu(\mathbf{r}_1)\varphi_\nu(\mathbf{r}_1)
  \frac{1}{\|\mathbf{r}_1 - \mathbf{r}_2\|}
  \varphi_\lambda(\mathbf{r}_2)\varphi_\sigma(\mathbf{r}_2)
  \, d\mathbf{r}_1\,d\mathbf{r}_2.
\end{equation}
Here $\mu,\nu,\lambda,\sigma \in \{1,\dots,N_\mathrm{bas}\}$ index basis
functions. The ERI tensor is a rank-4 object with
$\Theta(N_\mathrm{bas}^4)$ elements, and it possesses several permutation
symmetries, such as
\begin{equation}
  (\mu\nu|\lambda\sigma)
  = (\nu\mu|\lambda\sigma)
  = (\mu\nu|\sigma\lambda)
  = (\lambda\sigma|\mu\nu).
\end{equation}
By grouping pairs of indices into compound indices, e.g.\ $(\mu\nu)$ and
$(\lambda\sigma)$, one can reshape the ERI tensor into a
$N_\mathrm{bas}^2\times N_\mathrm{bas}^2$ matrix, which is convenient for
numerical linear algebra operations and for applying low-rank and hierarchical
approximations.

The ERI tensor plays a central role in Hartree--Fock, post-HF correlation
methods, and density functional theory. Direct storage of all ERIs scales as
$\Theta(N_\mathrm{bas}^4)$, and the naive computation of all integrals has a
similar or worse cost, depending on the integral algorithm
\cite{Boys1950,HeadGordonPople1988}. This motivates a wide range of integral
screening and compression techniques, including density fitting (resolution of
the identity) \cite{Whitten1973,BeebeLinderberg1977}, Cholesky decomposition of
the ERI matrix \cite{Aquilante2010}, and tensor hypercontraction
\cite{HohensteinMartinez2012}. These methods seek to reduce both the storage
cost and the asymptotic complexity of forming Coulomb and exchange contributions
by exploiting approximate low-rank structure and the locality of basis
functions.

\subsection{Hartree--Fock method}
\label{subsec:hf}

The Hartree--Fock (HF) method provides a mean-field approximation to the
electronic ground state by assuming that the $N$-electron wave function is a
single Slater determinant built from $N$ spin-orbitals. In its spin-restricted,
closed-shell form, HF minimizes the electronic energy with respect to a set of
occupied spatial orbitals $\{\phi_i\}$, subject to orthonormality constraints
\cite{SzaboOstlund1989,HelgakerJorgensenOlsen2000}. This leads to the
Roothaan--Hall equations
\begin{equation}
  \mathbf{F} \mathbf{C} = \mathbf{S} \mathbf{C} \mathbf{\varepsilon},
\end{equation}
where $\mathbf{F}$ is the Fock matrix, $\mathbf{S}$ is the overlap matrix,
$\mathbf{C}$ is the coefficient matrix of molecular orbitals in the atomic
orbital (AO) basis, and $\mathbf{\varepsilon}$ is the diagonal matrix of orbital
energies.

In an AO basis $\{\varphi_\mu\}$, the Fock matrix elements are given by
\begin{equation}
  F_{\mu\nu}
  = h_{\mu\nu}
    + \sum_{\lambda\sigma} P_{\lambda\sigma}
      \left[ 2 (\mu\nu|\lambda\sigma) - (\mu\lambda|\nu\sigma) \right],
\end{equation}
where $h_{\mu\nu}$ are one-electron integrals (kinetic energy and
electron--nuclear attraction), $P_{\lambda\sigma}$ is the density matrix
constructed from occupied orbitals, and $(\mu\nu|\lambda\sigma)$ are electron
repulsion integrals (ERIs). The HF procedure iterates between building the Fock
matrix from a trial density matrix and solving the generalized eigenvalue
problem until self-consistency is reached (self-consistent field, SCF).

The computational bottleneck of conventional HF lies in the construction of the
Coulomb and exchange contributions to the Fock matrix. Without any screening or
compression, the number of significant ERIs scales as
$\Theta(N_\mathrm{bas}^4)$, and each SCF iteration has a formal cost of
$\mathcal{O}(N_\mathrm{bas}^4)$ or higher, where $N_\mathrm{bas}$ is the number
of basis functions \cite{SzaboOstlund1989,HelgakerJorgensenOlsen2000}. Various
techniques have been developed to reduce this cost, including integral
prescreening, density fitting (resolution of the identity)
\cite{Whitten1973,BeebeLinderberg1977}, Cholesky decomposition of the ERI
matrix \cite{Aquilante2010}, and local or linear-scaling HF methods that exploit
the decay of the density matrix with distance and the nearsightedness of
electrons in insulators \cite{Yang1991,Goedecker1999}. Many of these techniques
are compatible with low-rank and hierarchical representations of the ERI tensor,
which further reduce the asymptotic complexity of HF calculations.

\subsubsection{Hierarchical block low-rank and $\mathcal{H}^2$-based ERI representations}
\label{subsubsec:hf-h2-eri}

A more recent line of work by Xing, Huang, and Chow combines the Hartree--Fock
framework with hierarchical block low-rank and $\mathcal{H}^2$-matrix
techniques to obtain near-linear-scaling algorithms for constructing the
Coulomb and exchange matrices.

In their J.~Chem.~Phys.~paper, Xing, Huang, and Chow introduced a
\emph{hierarchical block low-rank representation} of the ERI tensor
\cite{xing2020}. By reshaping the ERI tensor into a matrix with compound
indices $(\mu\nu)$ and $(\lambda\sigma)$, they treat it as a kernel matrix
associated with the Coulomb interaction between products of AO basis functions.
The AO indices are organized into a hierarchical cluster tree, and a
block-cluster tree is used to partition the ERI matrix into near-field and
far-field blocks. Near-field blocks are stored explicitly, while far-field
blocks are approximated in low-rank form using hierarchical block low-rank
techniques closely related to $\mathcal{H}^2$-matrices. This leads to a
data-sparse representation of the ERI tensor in which both storage and
matrix-vector multiplications scale linearly with the matrix dimension (up to
logarithmic factors), rather than quadratically as in the dense case
\cite{xing2020}.

To efficiently construct such $\mathcal{H}^2$-type representations, Huang,
Xing, and Chow developed the H2Pack library for kernel matrices
\cite{huang2020toms}. H2Pack uses a hybrid analytic--algebraic compression
strategy based on the proxy point method to build $\mathcal{H}^2$-matrix
representations with linear complexity in the number of points. Storage and
matrix-vector multiplication costs are both $\mathcal{O}(N)$, where $N$ is the
matrix dimension, under standard assumptions on the kernel and the geometry of
the points \cite{huang2020toms}. Although H2Pack is a general-purpose kernel
matrix package, its underlying ideas can be directly applied to the ERI matrix
viewed as a Coulomb kernel matrix over AO products.

Within the HF context, the hierarchical block low-rank ERI representation
enables the Coulomb and exchange contributions to the Fock matrix to be
constructed using only matrix-vector and small dense-matrix operations with the
compressed ERI representation. As a result, the asymptotic cost of building the
Fock matrix can be reduced from $\Theta(N_\mathrm{bas}^4)$ to nearly linear in
$N_\mathrm{bas}$ for large three-dimensional systems, while controlling the
approximation error through the ranks and tolerance parameters in the
hierarchical compression \cite{xing2020}. From the perspective of this work,
these results illustrate how hierarchical and $\mathcal{H}^2$-based low-rank
representations of the ERI tensor can substantially improve the asymptotic
behavior of mean-field electronic-structure calculations, and they provide an
important point of comparison for the tensor and hierarchical approaches
developed later in this paper.

\subsection{Second-order M{\o}ller--Plesset perturbation theory (MP2)}
\label{subsec:mp2}

Second-order M{\o}ller--Plesset perturbation theory (MP2) is one of the
simplest and most widely used post-HF correlation methods. Starting from a
converged HF reference, MP2 treats the residual electron correlation as a
second-order perturbation \cite{MollerPlesset1934}. In the canonical molecular
orbital (MO) basis, the MP2 correlation energy can be written as
\begin{equation}
  E_\mathrm{MP2}
  = \sum_{ijab}
    \frac{(ij||ab)^2}{\varepsilon_i + \varepsilon_j - \varepsilon_a - \varepsilon_b},
\end{equation}
where $i,j$ index occupied MOs, $a,b$ index virtual MOs, $\varepsilon_p$ are
orbital energies, and $(ij||ab)$ are antisymmetrized two-electron integrals in
the MO basis:
\begin{equation}
  (ij||ab) = (ij|ab) - (ij|ba).
\end{equation}

A straightforward evaluation of $E_\mathrm{MP2}$ scales as
$\mathcal{O}(N_\mathrm{occ}^2 N_\mathrm{vir}^2 N_\mathrm{bas})$ in floating
point operations and requires $\Theta(N_\mathrm{occ} N_\mathrm{vir}
N_\mathrm{bas}^2)$ storage if all relevant ERIs are precomputed, where
$N_\mathrm{occ}$ and $N_\mathrm{vir}$ are the numbers of occupied and virtual
MOs, respectively, and $N_\mathrm{bas}$ is the number of basis functions. In
terms of a single size parameter $N$ representing system or basis size, the
overall scaling is typically described as $\mathcal{O}(N^5)$ in time and
$\mathcal{O}(N^4)$ in memory \cite{SzaboOstlund1989,HelgakerJorgensenOlsen2000}.

Because of this unfavorable asymptotic behavior, a large literature has focused
on reducing the scaling of MP2. Existing approaches include local correlation
methods, which exploit the spatial locality of occupied orbitals and electron
pairs \cite{SaeboPulay1993,SchutzWerner2001}, density fitting and Cholesky
decomposition of the ERI tensor
\cite{Hattig2005,Aquilante2010,Weigend2002}, Laplace-transformed MP2
\cite{Hattig2005,AyalaScuseria1999}, and tensor-factorization techniques such
as resolution-of-the-identity MP2 (RI-MP2) and tensor hypercontraction
\cite{HohensteinMartinez2012}. Many of these methods can achieve effective
scaling closer to $\mathcal{O}(N^4)$ or even lower for large, insulating
systems, while maintaining chemical accuracy.

In this work, MP2 primarily serves as a representative example of a correlated
wave function method whose computational cost is dominated by operations
involving the ERI tensor. The asymptotic behavior of the MP2 energy evaluation
is therefore closely tied to the structure and compression of the ERI tensor
introduced in Section~\ref{subsec:eri-tensor}.