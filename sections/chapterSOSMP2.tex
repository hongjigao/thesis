\chapter{Hierarchically Compressed SOS-MP2 Algorithm}
\label{chap:h2mp2}

In this chapter we describe a hierarchically compressed spin-opposite-scaled
(SOS-) MP2 algorithm based on the work of Gao, Jiao, and
Levine~\cite{GaoJiaoLevineH2MP2}. The method combines the atomic-orbital
(AO) Laplace-transformed formulation of the MP2 energy with an $\mathcal{H}^2$
representation of the electron repulsion integral (ERI) tensor. The key idea is
to exploit both the data-sparsity of the ERI tensor in $\mathcal{H}^2$ form and
the element-wise sparsity of the energy-weighted density and complementary
density matrices.

\section{From closed-shell MP2 to the AO--Laplace trace form}
\label{sec:AOLaplaceMP2_trace}

We outline the algebraic steps that lead from the conventional closed-shell MP2
energy expression to an AO--Laplace formulation that can be evaluated via
trace contractions. This derivation provides the analytic target that the
hierarchical algorithm in Algorithm~\ref{alg:HSM algorithm} accelerates.

\paragraph{Closed-shell MP2 energy.}
In canonical molecular orbitals (MOs), the closed-shell MP2 correlation energy
may be written as
\begin{equation}
E_{\mathrm{MP2}}
=
-\sum_{ij}^{\mathrm{occ}}\sum_{ab}^{\mathrm{vir}}
\frac{(ia|jb)\,\bigl[2(ia|jb)-(ib|ja)\bigr]}
{\epsilon_a+\epsilon_b-\epsilon_i-\epsilon_j},
\label{eq:mp2_closed_shell}
\end{equation}
where \((pq|rs)\) are two-electron repulsion integrals (ERIs) in chemists'
notation and \(\Delta_{ij}^{ab}=\epsilon_a+\epsilon_b-\epsilon_i-\epsilon_j>0\)
denotes the orbital energy gap.

\paragraph{Laplace transform and quadrature.}
Using the Laplace identity \(1/\Delta=\int_0^\infty e^{-\Delta t}\,dt\) and a
quadrature approximation with \(\{t_\alpha,w_\alpha\}_{\alpha=1}^{n_q}\), we obtain
\begin{equation}
E_{\mathrm{MP2}}
\approx
\sum_{\alpha=1}^{n_q} w_\alpha\, e_2^\alpha,
\qquad
e_2^\alpha
=
-\sum_{ijab}
(ia|jb)\,\bigl[2(ia|jb)-(ib|ja)\bigr]\,
e^{-\Delta_{ij}^{ab}t_\alpha}.
\label{eq:mp2_laplace_quadrature}
\end{equation}
Since \(e^{-\Delta_{ij}^{ab}t_\alpha}
=
e^{+\epsilon_it_\alpha}e^{+\epsilon_jt_\alpha}e^{-\epsilon_at_\alpha}e^{-\epsilon_bt_\alpha}\),
we define at each quadrature point \(\alpha\) the energy-weighted density and
complementary density matrices in the AO basis,
\begin{align}
X_{\mu\nu}^{\alpha}
&=
\sum_{i}^{\mathrm{occ}} C_{\mu i}C_{\nu i}\,e^{+\epsilon_i t_\alpha},
\label{eq:X_def_trace}
\\
Y_{\mu\nu}^{\alpha}
&=
\sum_{a}^{\mathrm{vir}} C_{\mu a}C_{\nu a}\,e^{-\epsilon_a t_\alpha},
\label{eq:Y_def_trace}
\end{align}
where \(C\) denotes the MO coefficient matrix.

\paragraph{AO formulation via index-transformed ERIs.}
Let \(W\) denote the AO ERI tensor \(W_{\mu\nu\lambda\sigma}\equiv(\mu\nu|\lambda\sigma)\).
After applying the Laplace transform and quadrature, the MP2 energy becomes a weighted sum over quadrature points. In the remainder of this section we fix \emph{one} quadrature point and, for notational simplicity, drop the quadrature index \(\alpha\) on all quantities (i.e., \(X\equiv X^\alpha\), \(Y\equiv Y^\alpha\), etc.).

At the chosen quadrature point, the Laplace factors are absorbed into the energy-weighted density matrix \(X\) and complementary density matrix \(Y\) (Eq. ~\eqref{eq:X_def_trace}--\eqref{eq:Y_def_trace}). In the notation used
throughout this chapter and in Algorithm~\ref{alg:HSM algorithm}, we perform two
successive \emph{ket-side} index transformations:
\begin{align}
(\mu\nu|\underline{\lambda}\epsilon)
&\equiv
\sum_{\kappa}(\mu\nu|\kappa\epsilon)\,X_{\kappa\lambda},
\label{eq:half_transform_unified}
\\
(\mu\nu|\underline{\lambda}\overline{\sigma})
&\equiv
\sum_{\epsilon}(\mu\nu|\underline{\lambda}\epsilon)\,Y_{\epsilon\sigma}.
\label{eq:full_transform_unified}
\end{align}
Here \(\underline{\lambda}\) and \(\overline{\sigma}\) indicate transformed (and, in
practice, threshold-truncated) indices induced by \(X\) and \(Y\), respectively.

Define the Kronecker product
\begin{equation}
Z \equiv X\otimes Y,
\label{eq:Z_def_unified}
\end{equation}
which acts on the \emph{column} (ket) pair index \((\lambda\sigma)\), and the one-sided
index-transformed ERI matrix
\begin{equation}
T \equiv W Z,
\qquad
T_{(\mu\nu),(\lambda\sigma)} \equiv (\mu\nu|\underline{\lambda}\overline{\sigma}).
\label{eq:T_def_unified}
\end{equation}
The fully index transformed ERI matrix is then expressed as
\begin{equation}
Q \equiv Z W Z.
\label{eq:Q_def_unified}
\end{equation}

With these definitions, the MP2 contribution can be written as two
trace contractions: a Coulomb contraction with \(W\) and an exchange contraction with a
permuted ERI matrix \(V\) whose entries are \(V_{(\mu\nu),(\lambda\sigma)}=(\mu\sigma|\lambda\nu)\),
i.e., \(V=\Pi(W)\) for a fixed permutation \(\Pi\) on pair indices,
\begin{equation}
e_2
=
-2\,\mathrm{Tr}(Q W)
\;+\;
\mathrm{Tr}(Q V).
\label{eq:e2_trace_two_terms_unified}
\end{equation}
We will later treat the Coulomb-like and exchange-like parts separately.
In the present work, we focus on the \emph{Coulomb} contraction
\(-2\,\mathrm{Tr}(Q W)\), because it admits a particularly efficient evaluation
and directly yields the term used in SOS-MP2.

By cyclicity of the trace we obtain the trace-of-square identity
\begin{equation}
\mathrm{Tr}(Q W)
=
\mathrm{Tr}(Z W Z W)
=
\mathrm{Tr}\!\left[(W Z)^2\right]
=
\mathrm{Tr}\!\left[T^2\right],
\label{eq:trace_square_unified}
\end{equation}
and therefore the Coulomb contribution at this quadrature point can be written as
\begin{equation}
e_J
\equiv
-2\,\mathrm{Tr}(Q W)
=
-2\,\mathrm{Tr}\!\left[T^2\right].
\label{eq:eJ_trace_square_unified}
\end{equation}

In summary, the AO--Laplace formulation converts the Coulomb integral MP2 energy into a small number of trace contractions per quadrature point. The hierarchical method developed in this chapter is designed to accelerate the construction of \(T\) and the evaluation of \(\mathrm{Tr}\!\left[T^2\right]\)
(and, when desired, the analogous permuted contraction involving \(V\)).

\section{Methodology}
\label{sec:Methodology}
Here we describe the key steps in our hierarchical SOS-MP2 algorithm, which leverages both the data-sparsity of the ERI tensor in the $\mathcal{H}^2$ format and the element-wise sparsity of the energy-weighted density matrices. The method involves three main stages: partitioning the ERI tensor, performing index transformations on its short- and long-range components, and finally computing the SOS-MP2 energy. The overall procedure is presented in Algorithm~\ref{alg:HSM algorithm}.

\begingroup
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
% \begin{algorithm}[H]
% --- Start of the "framed" algorithm block ---
\noindent\rule{\linewidth}{0.8pt} % Top rule
\vspace{-30pt} % Adjust space to bring caption closer to the rule
\captionof{algorithm}{Hierarchical SOS-MP2 algorithm}
\label{alg:HSM algorithm}
\vspace{-15pt} % Adjust space to bring the next rule closer
\noindent\rule{\linewidth}{0.4pt} % A thinner rule below the caption

\begin{algorithmic}[1]
\Require The atomic orbital ERI tensor $W$ in $\mathcal{H}^2$ format. The coefficient matrix $C$ and occupied and virtual molecular orbitals $\epsilon_i$. The quadrature weights and abscissa $\{w_{\alpha}, t_{\alpha}\}$
\Ensure MP2 Coulomb-like term energy $E_2^{\text{SOS-MP2}}$
\For{$\alpha$ in quadrature points}
\State Calculate $X_{\mu\nu}^{\alpha} = \sum_{i}^{\text{occ}} C_{\mu i}C_{\nu i} e^{\epsilon_{i} t_{\alpha}}$ and  $Y_{\mu\nu}^{\alpha} = \sum_{a}^{\text{vir}} C_{\mu a} C_{\nu a} e^{-\epsilon_{a} t_{\alpha}}$
\State Do $X$ and $Y$ index transformations on $W_s$ to get transformed short-range part $T_s$  %row by row
\For{Block $B_i$ in completely low-rank format}
\For{Column $\kappa\epsilon$ in the block}
\For{$\lambda$ where $ X_{\kappa \underline{\lambda}}$ is the truncated significant elements}
\State Compare the block containing column $\underline{\lambda}\epsilon$ with $B_i$
\If {They are in the same level}
\State {Directly multiply the column basis set using \eqref{eq:case1}}
\ElsIf{the transformed block is of lower level}
\State{Do ancestor index transformation using \eqref{eq:case2}}
\Else
\State{Do descendant index transformation using \eqref{eq:case3}}
\EndIf
\EndFor
\EndFor
\EndFor
\State Get the half index transformed low-rank part ERI tensor $(\mu \nu|\underline{\lambda} \epsilon)$
\State Similarly do $Y$ index transformation to get transformed long-range part $T_l=(\mu \nu|\underline{\lambda} \overline{\sigma})$
\State Loop over all the elements of $T_s$ to compute $\text{Tr}(T_s^2+2T_s T_l)$.
\State Loop over all the blocks of $T_l$ to compute $\text{Tr}(T_l^2)$.
\State Summarize this quadrature point: $e_{2}^{\alpha} = -2 \, \text{Tr}\left[(T_s^2) + (T_l^2) + 2 T_s \cdot T_l\right]$
\EndFor
\State \Return $E_2^{\text{SOS-MP2}}=\sum_{\alpha} w_{\alpha}e^{\alpha}_2$
\end{algorithmic}
% \end{algorithm}
% --- The Footer: The final rule ---
\vspace{-5pt} % Adjust space to bring caption closer to the rule
\noindent\rule{\linewidth}{0.8pt} % Bottom rule
% \bigskip % Add some space after the algorithm
\endgroup

\subsection{Partitioning of the ERI Tensor}
\label{subsec:Decomposition of The ERI Tensor}
The SCF procedure involves the multiplication of the ERI tensor by the density matrix, which can be understood as a matrix-vector product in the space of basis function pairs.  Such an operation is well-suited to the $\mathcal{H}^2$-matrix representation. By contrast, the SOS-MP2 method involves operations that resemble a matrix-matrix product, which is less ideal for the direct application of the $\mathcal{H}^2$ format. Nonetheless, for finite systems, the electron density decays exponentially with distance, and insulator-type systems exhibit the fastest decay.\cite{kohn1995density,kohn1996density} As a result, the density and density complementary matrices are expected to be sparse, which allows us to avoid a full, complex matrix-matrix multiplication. Furthermore, substantial cancellations observed in MP2 calculations suggest that accurate results can be achieved by focusing on the dominant elements in one matrix, thereby further simplifying the computation.

In what follows, we assume that the SCF phase of the calculation is complete, and therefore the orbital coefficient matrices, $C$, are already available. With $C$, the energy-weighted density and density complementary matrices, $X$ and $Y$, computed using Eqs.~\eqref{eq:X_def_trace} and \eqref{eq:Y_def_trace}, can be formed directly.
Both calculations have a time complexity of $\mathcal{O}(N^3)$. However, these computations are performed only once, and the associated prefactor is sufficiently small that the overall time complexity is negligible for the systems considered in this work.  $X$ and $Y$ are both sparse in larger systems.  We therefore store them in compressed sparse row (CSR) format.  Elements below a user-chosen threshold, $\eta$, are approximated as zero.

As described in Section~\ref{sec:AOLaplaceMP2_trace}, the Coulomb-like term of the MP2 energy can be expressed as a weighted sum over quadrature points, $E_{2} = -\sum_{\alpha}^{\tau} w_{\alpha} e_{2}^{\alpha}$. In Section~\ref{sec:Results}, we employ the quadrature rules developed by Braess and Hackbusch.\cite{braess2005approximation,takatsuka2008minimax} Typically, seven or eight quadrature points are used.

The Coulomb-like term of the MP2 energy can be written explicitly as
\begin{equation}
  e_{2} = -2 \sum_{\mu,\nu,\lambda,\overline{\sigma},\gamma,\delta,\kappa,\epsilon} (\mu \nu | \lambda \sigma) X_{\mu\gamma} Y_{\nu \delta} (\gamma \delta | \kappa \epsilon) X_{\kappa \lambda} Y_{\epsilon \sigma}.
\end{equation}
Using the notations introduced in Section~\ref{sec:AOLaplaceMP2_trace},
the energy expression simplifies further to
\begin{equation}
    e_{2} = -2 \, \text{Tr}(T^2).
\end{equation}

We refer to the operation of multiplying $W$ by $X \otimes Y$ as \emph{index transformation}. Here, $W$ denotes the original ERI tensor, while $T$ denotes the index-transformed ERI tensor. As discussed in Section~\ref{subsec:hf-h2-eri}, the ERI tensor, represented as an $\mathcal{H}^2$-matrix, can be decomposed into a short-range component, $W_s$, which includes diagonal and near-diagonal blocks, and a long-range component, $W_l$, including the remainder of the matrix. Owing to the linearity of matrix-matrix multiplication,
\begin{equation}
    T=T_s+T_l
\end{equation}
where
\begin{equation}
\label{eq:shortrangeindextransform}
    T_s = W_s (X \otimes Y)
\end{equation}
and
\begin{equation}
\label{eq:longrangeindextransform}
    T_l = W_l (X \otimes Y).
\end{equation}
It is important to note that $T_s$ and $T_l$ do not denote the short- and long-range parts of the index-transformed ERI tensor.  Instead, they are the index-transformed versions of the short- and long-range parts of the original ERI tensor. The Coulomb-like term of the MP2 energy then takes the form
\begin{equation}
\label{eq:e2}
e_{2} = -2 \, \text{Tr}\left(T_s^2 + T_l^2 + 2 T_s T_l\right).
\end{equation}

\subsection{Index Transformation of the Short-Range Component}
\label{subsec:Index Transformation of the dense part}
Here we describe the transformation of the short-range component of the ERI tensor, Eq.~\eqref{eq:shortrangeindextransform}.
Both the original $W_s$ and the ultimate index-transformed $T_s$ are stored in CSR matrix format in order to leverage their sparsity, with elements whose absolute values fall below a second sparsification threshold, $\zeta$, are approximated as zero.
In practice, the transformation is carried out in two steps, applying $X$ and $Y$ to the ERI tensor separately. First, the $X$ index transformation is computed as
\begin{equation}
(\mu \nu | \underline{\lambda} \epsilon) = \sum_\kappa (\mu \nu | \kappa \epsilon) X_{\kappa \underline{\lambda}}.
\end{equation}
Following this transformation, the resulting intermediate ERI tensor is sparsified by applying the threshold $\zeta$.
 The $Y$ index transformation is then performed analogously
 \begin{equation}
 (\mu \nu | \underline{\lambda} \overline{\sigma}) = \sum_\epsilon (\mu \nu | \underline{\lambda} \epsilon) Y_{\epsilon \sigma}.
 \end{equation}



\subsection{Index Transformation of the Long-Range Component}
\label{subsec:Index Transformation of the low rank part}

Now we discuss the index transformation of the long-range component, Eq.~\eqref{eq:longrangeindextransform}.
The long-range part of the ERI, $W_l$, comprises a set of low-rank matrix blocks in an $\mathcal{H}^2$ representation. Since this index transformation applies a right multiplication to $W_l$, the nested property of the row basis sets is preserved, while the nested property in the column basis sets is lost.  Nevertheless, the row basis sets of blocks in the same row of blocks remain connected via transfer matrices.

We begin by converting the $\mathcal{H}^2$ matrix into a more general hierarchical matrix, no longer enforcing the nested property of the column basis.  This is done by retaining the row basis sets and multiplying the intermediate matrices by the column basis sets. This step ensures that the nested property of the row basis sets is maintained across different blocks.

Next, we perform the $X$ index transformation on the hierarchical matrix by evaluating
\[
(\mu \nu | \underline{\lambda} \epsilon) = \sum_\kappa (\mu \nu | \kappa \epsilon) X_{\kappa \underline{\lambda}},
\]
and store $(\mu \nu | \underline{\lambda} \epsilon)$ in a \emph{completely low-rank hierarchical matrix} format. Such a matrix is partitioned in the same manner as in the original hierarchical matrix.  However, all blocks, including the diagonal and neighboring blocks, are stored in low-rank format. Because right multiplication does not affect the row basis, each block’s row basis set in $(\mu \nu | \underline{\lambda} \epsilon)$ matches that in $(\mu \nu | \kappa \epsilon)$. We then iterate over all blocks in $(\mu \nu | \underline{\lambda} \epsilon)$, including both diagonal and neighboring blocks, to compute their column basis sets. Each column basis represents a basis function pair $\underline{\lambda} \epsilon$. When determining the influence of the row elements, $X_{\kappa \underline{\lambda}}$, on the column basis sets, we consider three cases based on the relationship between the row basis sets:

\begin{description}[leftmargin=0pt, labelwidth=\widthof{\textbf{Descendant Index Transformation}}]

  \item[\textbf{Same-level Index Transformation}] \par
  The row basis set of the block containing $\kappa \epsilon$ is identical to that of the block containing $\underline{\lambda} \epsilon$.

  \item[\textbf{Ancestor Index Transformation}] \par
  The row basis set of the block containing $\kappa \epsilon$ is a subset of the row basis set of the block containing $\underline{\lambda} \epsilon$.

  \item[\textbf{Descendant Index Transformation}] \par
  The row basis set of the block containing $\kappa \epsilon$ is a superset of the row basis set of the block containing $\underline{\lambda} \epsilon$.

\end{description}

Let $y$ denote the column basis vector $\underline{\lambda} \epsilon$ to be computed, and let $x$ denote the column basis set in $\kappa \epsilon$. When the block containing $\kappa \epsilon$ lies in the short-range part, it is treated as an empty block, as this part has already been computed in Section~\ref{subsec:Index Transformation of the dense part}.

In the same-level transformation, we directly multiply the value $X_{\kappa \underline{\lambda}}$ with the column basis vector $x$ to obtain the contribution to $y$, since the row basis sets of both blocks are identical. This can be written as
\begin{equation}
  y = X_{\kappa \underline{\lambda}} x.
  \label{eq:case1}
\end{equation}
This step is implemented in line 9 of Algorithm~\ref{alg:HSM algorithm}.

In the ancestor transformation, we trace the sequence of ancestors $n_i$ in the row tree from the block containing $\kappa \epsilon$ to the block containing $\underline{\lambda} \epsilon$. The row basis set of the block containing $\kappa \epsilon$ is effectively the recursive product of the transfer matrices along this ancestor sequence. We express this as
\begin{equation}
  y = X_{\kappa \underline{\lambda}} \prod_{i} R_{n_i} x.
  \label{eq:case2}
\end{equation}
   This step is implemented in line 11 of Algorithm~\ref{alg:HSM algorithm}.

In the descendant transformation, the procedure is more involved. First, we identify the blocks containing the required column $\kappa \epsilon$, denoted by a set of blocks, ${B_i}$, with corresponding row basis sets ${U_i}$ and columns ${x_i}$. For each $U_i$, we then trace a sequence of ancestors, $n_{ij}$, in the row tree from the block containing $\underline{\lambda} \epsilon$ down to the block containing $U_i$. This sequence is the reverse of that in the ancestor transformation, moving from descendant to ancestor rather than vice versa. We write
\begin{equation}
  y = \sum_{i} X_{\kappa \underline{\lambda}} \prod_{j} R^{-1}_{n_{ij}} x_i,
  \label{eq:case3}
\end{equation}
where $R^{-1}_{n_{ij}}$ denotes the pseudo-inverse of the transfer matrix, which is precomputed.

The $Y$ index transformation step proceeds analogously to the $X$ transformation step. The key difference is that diagonal and neighboring blocks are no longer treated as empty but instead are represented as low-rank matrices sharing the same row basis sets as their neighbors.

 In practice, the error introduced by the long-range index transformation is found to be smaller than that of the short-range transformation, which allows a higher threshold to be used when sparsifying $X$ and $Y$ compared to the threshold used for the short-range part of the index transformation.

\subsection{Computation of MP2 Energy}
\label{subsec:Computation of MP2 Energy}
With the CSR matrix, $T_{\text{s}}$, and the completely low-rank hierarchical matrix, $T_{\text{l}}$, representing the short- and long-range components of the index-transformed ERI tensor, the Coulomb-like term of the MP2 energy is given by \eqref{eq:e2}.
The terms $\text{Tr}(T_{\text{s}}^2)$ and $\text{Tr}(T_{\text{s}} T_{\text{l}})$ are evaluated directly by iterating over all elements of $T_{\text{s}}$ and identifying the corresponding elements in the other matrix to compute their contributions to the total trace.

For the term $\text{Tr}(T_{\text{l}}^2)$, due to the symmetric block structure of the hierarchical matrix, it is sufficient to select each block and compute the trace of the product of the block with its mirror image across the diagonal.

\subsection{Time and Space Complexity}
\label{subsec:complexity analysis}
\textcolor{black}{We analyze costs \emph{per quadrature point}, with the understanding that the number of points $\tau$ is a small constant ($\sim\!7$--$8$) and hence contributes only a constant factor overall. We adopt the following assumptions (standard for insulating finite systems and $\mathcal{H}^2$ compression): (A1) off-diagonal entries of the density and complementary matrices decay exponentially with the distance between basis-function centers;\cite{kohn1995density,kohn1996density} (A2) after thresholding at levels $\eta_s$ and $\eta_l$, the \emph{expected} number of significant entries per row/column of $X$ and $Y$ is $\mathcal{O}(1)$; (A3) the $\mathcal{H}^2$ representation of $W_l$ uses admissible blocks with numerical rank bounded by a geometry-dependent constant $k_{\max}$; (A4) let $m$ be the number of basis-function pairs retained after Schwarz screening ($m \le N^2$), the hierarchical partition over these $m$ pairs has $\mathcal{O}(\log m)$ levels with constant-size leaves; (A5) transfer matrices are well-conditioned so their (pseudo-)inverses are stable and bounded in size; (A6) formation of $X$ and $Y$ is treated as given for the purpose of asymptotic bounds (e.g., produced during or after SCF using standard linear-scaling routines), i.e., it does not dominate the costs reported below.}

\textcolor{black}{\textbf{Short-range index transformation.} A naive dense application of $X$ and $Y$ would be $\mathcal{O}(N^4)$, but by (A2) only $\mathcal{O}(1)$ entries per row/column of $X$ and $Y$ are retained in expectation. Applying these to $W_s$ yields $T_s$ with $\mathcal{O}(m)$ nonzeros and cost $\mathcal{O}(m)$; storage for $T_s$ in CSR is $\mathcal{O}(m)$.}

\textcolor{black}{\textbf{Long-range index transformation.} Converting $W_l$ from $\mathcal{H}^2$ to a general hierarchical form by folding column bases into intermediates costs $\mathcal{O}(m\log m)$. The $X$- and $Y$-steps each require assembling column bases across all levels. By nearsightedness and the level-wise geometry of the trees, the expected number of significant interactions per target is $\mathcal{O}(1)$, while there are $\mathcal{O}(m)$ targets per level and $\mathcal{O}(\log m)$ levels. Hence the total expected work is $\mathcal{O}(m\log m)$ per step; storage for the completely low-rank result is also $\mathcal{O}(m\log m)$. A detailed derivation of this $\mathcal{O}(m\log m)$ bound for the long-range step is provided in the Supplementary Material.}

\textcolor{black}{\textbf{Energy accumulation.} Evaluating $\text{Tr}(T_s^2)$ and $\text{Tr}(2T_s T_l)$ by iterating over the $\mathcal{O}(m)$ nonzeros of $T_s$ costs $\mathcal{O}(m)$. The term $\text{Tr}(T_l^2)$ involves level-wise block products with $\mathcal{O}(m)$ work per level and $\mathcal{O}(\log m)$ levels, i.e., $\mathcal{O}(m\log m)$.} \textcolor{black}{Combining the above, the per-quadrature \emph{work} and \emph{storage} are
 \begin{equation}
\#(\text{operations}) = \mathcal{O}(m \log m), \qquad \#(\text{storage}) = \mathcal{O}(m \log m),
    \label{eq:complexity}
 \end{equation}
which implies the worst-case bounds $\mathcal{O}(N^2\log N)$ since $m\le N^2$.}

\subsection{Error Analysis}
\label{subsec:Error Analysis}
Now we turn our attention to analyzing the sources of numerical error in our algorithm.  The errors in the index transformation procedure primarily arise from neglecting small elements of $X$ and $Y$ to enable sparse storage. Let $X_{\text{r}}$ and $Y_{\text{r}}$ denote the truncated matrices. The resulting error in $T_{\text{s}}$ comprises the terms
\begin{equation}
T_{\text{sX}} = W_{\text{d}} (X_{\text{r}} \otimes Y),
\end{equation}
\begin{equation}
T_{\text{sY}} = W_{\text{d}} (X \otimes Y_{\text{r}}),
\end{equation}
and
\begin{equation}
T_{\text{sXY}} = W_{\text{d}} (X_{\text{r}} \otimes Y_{\text{r}}),
\end{equation}
where subscript $\text{r}$ indicates the residual after sparsification. Analogous considerations apply to $T_{\text{l}}$.
This error behaves similarly to rounding error and is controlled by the thresholds $\eta$ (for sparsifying $X$/$Y$) and $\zeta$ (for sparsifying the short-range transforms). If $T_{\text{sX}}$ and $T_{\text{sY}}$ are $\mathcal{O}(\epsilon)$, then $T_{\text{sXY}}$ is $\mathcal{O}(\epsilon^2)$ and typically negligible. In principle, one could reduce the net error from $\mathcal{O}(\epsilon)$ to $\mathcal{O}(\epsilon^2)$ by evaluating $\text{Tr}\!\left[T_{\text{s}}(T_{\text{sX}} + T_{\text{sY}})\right]$, \textcolor{black}{which would add only $\mathcal{O}(m)$ work}. However, in our experiments, the observed error is already sufficiently small and comparable to the intrinsic error in $\text{Tr}(T_{\text{s}}^2)$, which cannot be corrected as efficiently, so we omit this step.

Errors from the low-rank approximation and the low-rank index transformation are dominated by the descendant transformation, where pseudo-inverses of transfer matrices are used. With well-conditioned transfer matrices and bounded numerical ranks, this contribution is negligible. The Laplace transformation itself introduces additional numerical error that is well understood and typically much smaller than the MP2 model error.

\subsection{Potential for Parallel Implementation}
\label{subsec:parallel properties}
In quantum chemistry, along with physical approximations and efficient numerical methods, parallelization is a key strategy to extend the size and complexity of systems that may be studied.\cite{calvin2021chemrev,bernholdt1996largescale,fletcher1999parallel,hattig2006distributedmemory,ufimtsev2008gpu1,Fales2015,chow2015parallel,werner2015scalable,peng2016tiledarray,kowalski2021nwchemex,hu2024das}  That hierarchical matrices naturally map to massively parallel computer architectures is a significant advantage that will be exploited in future work.  In Laplace transform MP2, the computation of the MP2 energy at each quadrature point is independent, making the algorithm highly parallelizable. Furthermore, each row computation in $T_{\text{s}}$ and each block computation in $T_{\text{l}}$ are also independent, enabling parallelization of the short- and long-range parts of the index transformation, respectively. This same property applies to the MP2 energy computation step. In other words, the Hierarchical SOS-MP2 algorithm could theoretically achieve constant time complexity with an infinite number of processors. However, since parallelization requires considerable memory, we implemented parallelization only for the quadrature points in this proof-of-concept paper. \textcolor{black}{We apply OpenMP with 8 threads on this parallelization because there are usually 7 or 8 number of quadrature points in the Laplace transformation. } Future work will involve parallelizing other parts of the algorithm on massively parallel computers.

\section{Implementation details}
\label{sec:ImplementationDetails}

\paragraph{Computational setup and test systems.}
All computations were carried out on the Seawulf cluster at Stony Brook
University (dg-mem node).  We selected two model systems: (i) linear alkane
chains as representative one-dimensional systems and (ii) water clusters as
representative three-dimensional systems.  Unless stated otherwise, all
SOS-MP2 calculations employed the cc-pVDZ basis set.  For sparsification of
both the short-range ERI tensor and the short-range index-transformed tensor,
we used a fixed threshold \(\zeta = 1\times 10^{-6}\).
The sparsification thresholds for the energy-weighted density matrices \(X\)
and \(Y\) are allowed to differ between the short- and long-range
transformations, denoted by \(\eta_s\) and \(\eta_l\), respectively.  Except
where noted, we used \(\eta_s=10^{-5}\) and \(\eta_l=10^{-4}\) for the alkane
series, and \(\eta_s=\eta_l=3\times 10^{-4}\) for the water clusters.

\paragraph{Code base and data structures.}
The entire prototype implementation is written in C for fine-grained control
over memory layout and traversal over sparse and hierarchical data structures.
The hierarchical ERI tensor \(W\) is represented using the same \(\mathcal{H}^2\)
data model as in the existing \textsc{H2P-ERI} infrastructure; however, the
in-memory representation used by the SOS-MP2 routines is more elaborate than a
minimal textbook \(\mathcal{H}^2\) format due to (i) symmetry exploitation,
(ii) mixed indexing conventions required by the index transformations, and
(iii) the need to efficiently query individual entries during trace
accumulation.

\paragraph{Pair-index space: basis-function pairs and spatial blocking.}
We view the ERI tensor as a matrix over \emph{basis-function pairs} (BFPs),
i.e.,
\(
W_{(\mu\nu),(\lambda\sigma)} \equiv (\mu\nu|\lambda\sigma),
\)
so each row and column index corresponds to one AO pair.  The row and column
cluster trees are built by recursively partitioning three-dimensional space
into axis-aligned boxes; at each level, one box is subdivided into eight child
boxes (an octree).  Each leaf box contains a set of \emph{screened shell pairs}.  A
shell pair is assigned to a box according to the geometric midpoint of the two
shell centers.  If the two shells contain \(M\) and \(N\) basis functions, then
their shell pair contributes \(MN\) BFPs; consequently, the number of BFPs in a
box is the total number of basis-function pairs induced by its shell pairs.
This geometric decomposition defines the hierarchical blocking of \(W\).  The
row and column trees interact through an admissibility criterion to determine
which block pairs are stored as near-field (short-range) blocks and which are
represented in low-rank form, thereby yielding an \(\mathcal{H}^2\) matrix for
the ERIs.

\paragraph{\(\mathcal{H}^2\) storage: nested bases and block couplings.}
For an admissible (far-field) block \(B\) coupling a row cluster \(r\) and a
column cluster \(c\), we store a nested row basis \(U_r\), a nested column
basis \(U_c\), and a small coupling matrix \(S_{rc}\) (denoted \(B\) in some
\(\mathcal{H}^2\) literature).  The numerical ranks are small and bounded in
practice, so \(S_{rc}\in\mathbb{R}^{k_r\times k_c}\) is typically tiny compared
to the full block.  The block entries are represented as
\begin{equation}
W_{rc} \approx U_r \, S_{rc} \, U_c^{\mathsf{T}}.
\end{equation}
This representation exploits standard ERI symmetries.  In particular, because
\((\mu\nu|\lambda\sigma)=(\mu\nu|\sigma\lambda)\), the BFP index \((\lambda\sigma)\)
can be treated as an \emph{unordered} pair in the stored ERI operator.  This
reduces storage and construction costs substantially for the base ERI tensor.

\paragraph{Ordered pair indices for index transformation.}
In the SOS-MP2 contractions, the column (ket) side undergoes index
transformations with \(X\) and \(Y\).  These transformations are sensitive to
the ordering of the two functions in a pair index, so an unordered BFP is no
longer sufficient on the transformed side.  To address this, we introduce an
\emph{ordered} pair-index representation, which we refer to as a
basis-function list (BFL): each original BFP \((\lambda\sigma)\) is expanded
into two ordered variants, \([\lambda,\sigma]\) and \([\sigma,\lambda]\), except
when both functions belong to the same shell (in which case the distinction is
unnecessary).  Conceptually, this converts the column index space from BFPs
(unordered) to BFLs (ordered), enabling correct application of the \(X\) and
\(Y\) transformations.

\paragraph{Short-range implementation: CSR storage and BFL\(\rightarrow\)BFP compression.}
For the short-range part \(T_s = W_s (X\otimes Y)\), we store \(W_s\) and
\(T_s\) in compressed sparse row (CSR) format.  The short-range index
transformations are applied on a row-by-row basis:
we first form \((\mu\nu|\underline{\lambda}\epsilon)\) by applying \(X\) and
sparsify the intermediate with threshold \(\zeta\), then apply \(Y\) to obtain
\((\mu\nu|\underline{\lambda}\overline{\sigma})\) and sparsify again.
Internally, the transformed columns are naturally generated in BFL form due to
ordering; however, we immediately compress the final short-range result back
to the unordered BFP representation to reduce memory.  Specifically, the two
ordered variants associated with a single unordered BFP are summed into one
value.  This compression reduces the storage for \(T_s\) by nearly a factor of
two while preserving the trace-of-square target:
computing \(\mathrm{Tr}(T_s^2)\) on the compressed BFP representation is
equivalent to computing \(\mathrm{Tr}(\tilde{T}_s^2)\) on the corresponding
expanded BFL representation.  This equivalence allows all subsequent short-range
energy accumulation to proceed in the compact BFP index space.

\paragraph{Long-range implementation: \(\mathcal{H}^2\rightarrow\) hierarchical conversion and ordered transforms.}
For the long-range part \(T_l = W_l (X\otimes Y)\), direct right multiplication
would destroy the nested structure of the column bases in the original
\(\mathcal{H}^2\) representation.  We therefore convert \(W_l\) into a more
general hierarchical format before applying the transformations.  Concretely,
for each admissible block we absorb the coupling matrix into the column basis,
\begin{equation}
W_{rc} \approx U_r S_{rc} U_c^{\mathsf{T}}
\quad\Rightarrow\quad
W_{rc} \approx U_r \,\widehat{U}_{rc}^{\mathsf{T}},
\qquad
\widehat{U}_{rc}^{\mathsf{T}} \equiv S_{rc} U_c^{\mathsf{T}},
\end{equation}
so that each block now carries its own (non-nested) effective column factor
\(\widehat{U}_{rc}\), while blocks in the same block row continue to share the
same nested row basis \(U_r\).  The index transformations are then applied to
the block-specific column factors, producing a hierarchical (completely low-rank)
representation of \(T_l\).  In contrast to the short-range case, we apply the
\(X\) and \(Y\) transformations as two separate passes for the long-range part,
as this reduces the overall assembly and traversal cost in the hierarchical
data structure (see Section~\ref{subsec:Index Transformation of the low rank part}).

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth,height=0.95\textheight,keepaspectratio]{figures/flowchart.png}
\caption{Overall workflow of the SOS-MP2 implementation.}
\label{fig:flowchart}
\end{figure}

\noindent
Figure~\ref{fig:flowchart} summarizes the end-to-end dataflow of the
implementation. Starting from the ERI operator, we separate the short-range and
long-range contributions and apply the two right-side index transformations
defined by the energy-weighted density matrices $X$ and $Y$.
For the short-range path, the transformations are executed in a row-wise manner
on the CSR representation and the final transformed tensor is compressed back
to the unordered BFP index space to reduce memory.
For the long-range path, the $\mathcal{H}^2$ representation is first converted
to a blockwise low-rank form so that the ordered transforms can be applied to
block-specific column factors; the resulting $T_l$ is evaluated through
hierarchical traversals (same-level/ancestor/descendant interactions).
Finally, the SOS-MP2 Coulomb-like contribution is accumulated as
$e_2=-2\,\mathrm{Tr}(T_s^2 + T_l^2 + 2T_sT_l)$, where the cross term is obtained
by iterating over the nonzeros of $T_s$ and querying the corresponding entries
in the hierarchical representation of $T_l$.


\paragraph{Energy accumulation and cross term.}
With \(T_s\) stored as a CSR matrix over BFP indices and \(T_l\) stored as a
hierarchical low-rank matrix, we evaluate the Coulomb-like energy contribution
via
\(
e_2 = -2\,\mathrm{Tr}(T_s^2 + T_l^2 + 2T_sT_l).
\)
The terms \(\mathrm{Tr}(T_s^2)\) and \(\mathrm{Tr}(2T_sT_l)\) are accumulated by
iterating over the nonzeros of \(T_s\); for each element of \(T_s\), we query
the corresponding entry in \(T_l\) (via the hierarchical block lookup) and add
its contribution.  The term \(\mathrm{Tr}(T_l^2)\) is evaluated blockwise using
the symmetric structure of the hierarchical matrix: for each low-rank block,
we multiply it with its mirror block across the diagonal and accumulate the
trace of the product.

Overall, the implementation is designed to (i) reuse the \textsc{H2P-ERI}
hierarchical ERI representation, (ii) introduce ordered pair indexing only
where required by the SOS-MP2 index transformations, and (iii) keep the final short-range transformed tensor in a compact unordered BFP form so that both memory and trace accumulation costs remain low.

\section{Results}
\label{sec:Results}
\subsection{The Sparsity of Density and Density Complementary Matrices}
\label{subsec:The sparsity of density and density complementary matrices}

\begin{figure}[htbp]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=0.94\columnwidth]{figures/xyralk_l.eps}
        \caption{The number of significant values per row or column in the energy-weighted density matrices in the series of linear alkane chains.}
        \label{fig:sparsityalkane}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=0.94\columnwidth]{figures/xyrwater_l.eps}
        \caption{The number of significant values per row or column in the energy-weighted density matrices in the series of water clusters.}
        \label{fig:sparsitywater}
    \end{minipage}
\end{figure}

The performance of our algorithm is contingent on the sparsity of the energy-weighted density matrices $X$ and the density complementary matrices $Y$, thus we quantify that sparsity here.  We report the number of significant elements per row in Figure~\ref{fig:sparsityalkane} for the alkane chain and Figure~\ref{fig:sparsitywater} for the water cluster. Values were computed across all quadrature points, and the maximum value was retained. For the alkane system, we observe rapid growth in the number of significant values from $\mathrm{C}_{10}\mathrm{H}_{32}$ to $\mathrm{C}_{30}\mathrm{H}_{92}$. Beyond this range, growth slows and eventually saturates, consistent with the expected scaling of $\mathcal{O}(N)$ total significant density matrix element for large systems. The water cluster exhibits similar behavior---as the system size increases, the number of significant values per row reaches a plateau. These results confirm the asymptotic sparsity of both the density and complementary matrices in both 1-D and 3-D systems.

\begin{figure}[htbp]
  \centering
  \begin{minipage}[t]{0.48\textwidth}
      \centering
      \includegraphics[width=0.94\columnwidth]{figures/combinedalktime.png}
      \caption{The time-to-solution versus number of basis functions for the series of linear alkane chains.}
      \label{fig:alktime}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.48\textwidth}
      \centering
      \includegraphics[width=0.94\columnwidth]{figures/combined_wattime.png}
      \caption{The time-to-solution versus number of basis functions for the series of water clusters.}
      \label{fig:watertime}
  \end{minipage}
\end{figure}


\subsection{Time Complexity}
\label{subsec:Time complexity analysis}
The measured times-to-solution for a series of linear alkane chains is shown in Figure~\ref{fig:alktime}. All timing results are averaged over three runs. Molecular structures were rendered using Jmol.\cite{Jmol}

The first four data points correspond to systems $\mathrm{C}_{10}\mathrm{H}_{22}$ through $\mathrm{C}_{40}\mathrm{H}_{82}$, in which the $X$ and $Y$ matrices have not yet reached asymptotic sparsity. Beyond this point, the number of nonzero elements per row of $X$ and $Y$ stabilizes. To illustrate the role of sparsity, we include two trend lines. For sufficiently large systems, we expect time complexity to scale as $\mathcal{O}(N^2 \log N)$, though the observed performance is closer to $\mathcal{O}(N^{1.38})$.  The superior performance is likely due to Schwarz screening. For smaller systems, complexity grows more quickly, which is consistent with expectations. The observed $\mathcal{O}(N^{2.38})$ scaling is still better than the theoretical $\mathcal{O}(N^3 \log^2 N)$ bound.  A discontinuity is observed between $\mathrm{C}_{50}\mathrm{H}_{102}$ and $\mathrm{C}_{60}\mathrm{H}_{122}$, corresponding to an increase in the number of levels in the hierarchical tree. Despite this, the increase in time-to-solution remains nearly linear.

Times-to-solution for the water cluster system are shown in Figure~\ref{fig:watertime}. For clusters with 30--50 molecules (first five points), time complexity scales approximately as $\mathcal{O}(N^{2.66})$. For clusters with 50--70 molecules (last five points), scaling improves to $\mathcal{O}(N^{1.83})$. The absolute time cost is higher for the water clusters than for the alkanes, as expected for a 3-D system, in which a larger fraction of the interactions are short range.

It is worth noting that, for the largest water clusters, we modified the hierarchical block-splitting algorithm relative to what was used for smaller clusters and alkane chains. In 3-D systems, the number of interacting block pairs grows rapidly with system size. To mitigate this, we cap the maximum depth of the block tree to control the number of interactions. This change improves performance and preserves the desired asymptotic behavior.

 \textcolor{black}{We also tested different values of $\eta_s$ and $\eta_l$.  We find that the threshold also does not affect the asymptotic growth of the time complexity of the method.}

\begin{figure}[htbp]
  \centering
  \begin{minipage}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=0.94\columnwidth]{figures/combinedalkmem.png}
      \caption{The memory versus number of basis functions for the series of linear alkane chains.}
      \label{fig:alkmem}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=0.94\columnwidth]{figures/combined_watmem.png}
      \caption{The memory versus number of basis functions for the series of water clusters.}
      \label{fig:watermem}
  \end{minipage}
\end{figure}

\subsection{Space Complexity}
\label{subsec:Space complexity analysis}

The total memory storage needed for the alkane chain systems is shown in Figure~\ref{fig:alkmem}. Their behavior closely parallels the time complexity results, though the rate of growth is even slower. For systems smaller than $\mathrm{C}_{40}\mathrm{H}_{82}$, where $X$ and $Y$ have not yet reached asymptotic sparsity, storage scales as $\mathcal{O}(N^{1.68})$. For larger systems, the theoretical scaling is $\mathcal{O}(N^2 \log N)$, but observed performance is $\mathcal{O}(N^{1.22})$, approaching linear. As in the time complexity case, we observe a step increase between $\mathrm{C}_{50}\mathrm{H}_{102}$ and $\mathrm{C}_{60}\mathrm{H}_{122}$, again due to an increase in the number of levels in the hierarchical tree.

Figure~\ref{fig:watermem} presents the memory storage for the water clusters. For clusters with 30--50 molecules, the observed scaling is approximately $\mathcal{O}(N^{1.79})$, while storage scales as $\mathcal{O}(N^{1.64})$ for clusters with 50--70 molecules. In contrast to the alkane chain system, this scaling does not flatten substantially with increasing system size, but remains within acceptable bounds and significantly better than the theoretical bound.

We attribute the slowdown in the alkane chain's asymptotic growth to Schwarz screening. In 1-D systems, the number of atoms within a given radius of a particular atom is small, and the majority of atom pairs interact weakly. Schwarz screening is therefore highly effective in eliminating small terms, yielding reduced memory costs. In 3-D systems, more pairs of atoms are close enough to interact strongly, leaving fewer terms that can be ignored. As a result, memory cost does not flatten as dramatically.

\begin{figure}[htbp]
  \centering
  \begin{minipage}[t]{0.48\textwidth}
     \centering
     \includegraphics[width=0.94\columnwidth]{figures/erroralk.eps}
     \caption{The error for alkane chain, computed with default thresholds ($\eta_s = 10^{-5}$, $\eta_l = 10^{-4}$) and tighter thresholds ($\eta_s = 10^{-6}$, $\eta_l = 10^{-5}$), are shown in blue and orange, respectively.}
     \label{fig:err}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.48\textwidth}
     \centering
     \includegraphics[width=0.94\columnwidth]{figures/errorwt_line.eps}
     \caption{The error for water cluster using default thresholds.}
     \label{fig:errwat}
  \end{minipage}
\end{figure}


\subsection{Energetic Accuracy}
\label{subsec:Error analysis}

We have carried out error analysis only on small systems. To validate our method, we compute the Coulomb-like component of the MP2 energy using \textsc{Molpro}.\cite{werner2012molpro,werner2020molpro,werner2003fast}  These reference calculations were performed both exactly and with density fitting with default parameters and compared to the results from our hierarchical SOS-MP2 algorithm. Total error in energy relative to exact MP2 as a function of system size is presented for our alkane and water cluster models in Figures~\ref{fig:err} and \ref{fig:errwat}.  The observed error relative to exact MP2 is comparable to that of density fitting (DF), but achieved at significantly lower cost in both time and memory. Interestingly, the error in our results is not observed to grow linearly with system size. The dominant source of error in our implementation is due to thresholding of the $X$ and $Y$ matrices. This error behaves more like numerical rounding error and typically scales as $\mathcal{O}(\sqrt{N})$. Although this introduces some mild and irregular oscillations in the total error, it nonetheless grows more slowly than linearly.

The thresholds used for sparsification of $X$ and $Y$ can be adjusted to improve accuracy. For the time and space complexity studies presented in Sections~\ref{subsec:Space complexity analysis} and~\ref{subsec:Time complexity analysis}, we used the default thresholds listed above, which are already sufficiently small for most practical applications. However, as shown in Figure~\ref{fig:err}, this error can be reduced further by tightening the threshold by a factor of ten ($\eta_s = 10^{-6}$, $\eta_l = 10^{-5}$). Varying the threshold does not affect the qualitative trend in the sparsity growth of the $X$ and $Y$ matrices; instead, it simply shifts the onset of convergence.

For water cluster systems, we performed additional tests on smaller examples and found that the error remained quite small in practice. As illustrated in Figure~\ref{fig:errwat}, the error is notably smaller than that of DF---even though we employed relatively loose truncation thresholds for the $X$ and $Y$ matrices. We attribute this improved accuracy to cancellation effects within the sparsified matrices. Although each individual truncation introduces error, these errors tend to cancel out when summed over the full ERI contraction, resulting in significantly lower total error.

\begin{figure}[htbp]
  \centering
     \includegraphics[width=0.94\columnwidth]{figures/threserr.png}
     \caption{\textcolor{black}{The error and time-to-solution for a water cluster system with 35 water molecules as a function of both thresholds, which are set equal to one another, $\eta_s=\eta_l$.  The grey dashed line indicates the accuracy of DF-MP2 with default parameters.}}
     \label{fig:thres}
\end{figure}

Our experiments across multiple systems and threshold values reveal a sharp drop in threshold near $\eta_s=1 \times 10^{-4}$. Beyond that point, the impact of the threshold on the error slows considerably. By choosing a sparsification threshold of $\zeta = 1 \times 10^{-8}$~Hartree for the short-range part of the ERI tensor, and $\eta_s = 1 \times 10^{-8}$ for the $X$ and $Y$ matrices, the total error can be pushed below $1 \times 10^{-5}$~Hartree—comparable to the inherent numerical error of the Laplace transformation itself.  However, in most cases, an error of $1 \times 10^{-4}$~Hartree (roughly 0.06 kcal/mol) is entirely acceptable, especially given that this is far smaller than the intrinsic error of MP2 itself. Accordingly, we opted for a balanced choice of thresholds (detailed in Section~\ref{subsec:The sparsity of density and density complementary matrices}) when plotting and memory costs.  \textcolor{black}{Figure \ref{fig:thres} illustrates how the error and running time behave as a function of threshold ($\eta_s=\eta_l$) for a 35-molecule water cluster. When the threshold is relaxed, the error remains almost unchanged below a certain value, but increases rapidly once this critical point is passed. By contrast, the running time decreases gradually.}

While these empirical findings are promising, we currently lack a complete theoretical understanding of the underlying cancellation behavior. It is possible that some deeper symmetry in the structure of the ERI tensor and the $X$ and $Y$ matrices drives this error suppression. We intend to explore this question further, as it may reveal new opportunities for improved performance and accuracy.

\subsection{Terminal C--C bond stretching test on \textit{alkane32}}
\label{sec:alkane32_stretch}

To probe the stability of the predicted potential energy surface (PES) under small geometric perturbations, we performed a simple terminal bond-stretching scan on \textit{alkane32}. Specifically, the outermost (terminal) carbon atom was displaced along the terminal C--C bond direction by $\delta \in [0.01,\,0.10]~\text{\AA}$ (with an additional reference at $\delta=0$), and single-point correlation energies were evaluated at each stretched geometry. Table~\ref{tab:alkane32_stretch} reports the SOS-MP2 energies together with the H2P predictions (shown with a negative sign for easier comparison), as well as their differences $\Delta E = E_{\mathrm{SOS\text{-}MP2}} - E_{\mathrm{H2P}}$.

\begin{table}[t]
\centering
\small
\renewcommand{\arraystretch}{1.15}
\setlength{\tabcolsep}{7pt}
\caption{Correlation energies (Hartree) for \textit{alkane32} under terminal C--C bond stretching. H2P energies are displayed with a negative sign.}
\label{tab:alkane32_stretch}
\begin{tabular}{r r r r}
\toprule
$\delta$ (\AA) & $E_{\mathrm{SOS\text{-}MP2}}$ (Ha) & $E_{\mathrm{H2P}}$ (Ha) & $\Delta E$ (Ha) \\
\midrule
0.00 & -1.163197274 & -1.163344603 & 0.000147329 \\
0.01 & -1.163264495 & -1.163410874 & 0.000146379 \\
0.02 & -1.163333769 & -1.163480357 & 0.000146588 \\
0.03 & -1.163405054 & -1.163550115 & 0.000145061 \\
0.04 & -1.163478321 & -1.163622854 & 0.000144533 \\
0.05 & -1.163553544 & -1.163698278 & 0.000144734 \\
0.06 & -1.163630696 & -1.163772891 & 0.000142195 \\
0.08 & -1.163790691 & -1.163931823 & 0.000141132 \\
0.10 & -1.163958131 & -1.164100249 & 0.000142118 \\
\bottomrule
\end{tabular}

\vspace{2pt}
\footnotesize
$\Delta E = E_{\mathrm{SOS\text{-}MP2}} - E_{\mathrm{H2P}}$.
Although the absolute offset remains at the $\sim10^{-4}$~Ha level, the residual error in stretch-induced \emph{relative} energies stays around $\sim10^{-6}$~Ha (max $\approx 6.2\times10^{-6}$~Ha), which is advantageous for PES construction where energy differences dominate.
\end{table}

\section{Attempts on the exchange-like terms}

In the previous section we focused on the Coulomb-type contribution. Under the matrix formulation, that term can be interpreted as the trace of a ``matrix square'' (equivalently, a self-contraction of an ERI-derived matrix), which makes the algebraic structure and the computational treatment comparatively direct. In contrast, exchange-like terms involve a different index pairing and therefore do not immediately fit into the same square-trace viewpoint. In this section, we attempt to analyze the exchange integral and outline a decomposition strategy that leads to several tractable subterms. For clarity, the discussion below is written for the first quadrature point; the extension to all quadrature points is straightforward by summation.

We consider the exchange contribution in the form
\begin{equation}
e_{2,K}=\sum_{\mu\nu\lambda\sigma}(\mu\nu\mid\lambda\sigma)\,(\mu\sigma\mid\lambda\nu),
\label{eq:exchange_start}
\end{equation}
where the first factor $(\mu\nu\mid\lambda\sigma)$ should be understood as the quadrature-separated (pointwise) Coulomb-like integral at the chosen quadrature point.

As defined previously in~\eqref{eq:Z_def_unified} and the corresponding ERI matrix definition, we continue to use $W$ to denote the unfolded ERI matrix with entries $W_{(\mu\nu),(\lambda\sigma)}=(\mu\nu\mid\lambda\sigma)$, and $Z=X\otimes Y$ to denote the diagonal scaling matrix at the first quadrature point. With these notations, the quadrature-separated factor appearing in~\eqref{eq:exchange_start} is exactly the $(\mu\nu,\lambda\sigma)$-entry of $ZWZ$.

The exchange-like structure comes from the second factor $(\mu\sigma\mid\lambda\nu)$, which pairs indices differently from $W_{(\mu\nu),(\lambda\sigma)}$. To capture this pairing within the same matrix contraction framework, we introduce a cross-transposed ERI matrix $V$ whose entries are defined by the index permutation
\[
V_{(\lambda\sigma),(\mu\nu)} := (\mu\sigma\mid\lambda\nu).
\]

Equivalently, $V$ is obtained from $W$ by the entrywise relabeling
\[
(\mu,\nu,\lambda,\sigma)\mapsto(\mu,\sigma,\lambda,\nu),
\]
i.e., each entry in $W$ is paired with another entry whose indices are ``cross-transposed'' in the sense above. This cross-transpose pairing is illustrated in Fig.~\ref{fig:crosstranspose_exchange}. In that schematic, we use a small toy index range (e.g.\ $\mu,\nu,\lambda,\sigma\in\{1,\dots,4\}$) to visualize how an entry finds its partner under the map. For instance, the block labeled $A$ corresponds to the element indexed by $(\mu,\nu,\lambda,\sigma)=(1,1,1,2)$, and its paired element under the cross-transpose becomes $(\mu,\sigma,\lambda,\nu)=(1,2,1,1)$, which is labeled as $B$ in the figure. Similarly, the entry labeled $C$ corresponds to $(1,3,2,2)$ and is mapped to $(1,2,2,3)$, labeled as $D$. These examples show explicitly how one can identify the paired indices and thus realize the transformation from $W$ to $V$ at the matrix-entry level.

Then, following the same ``sum-to-trace'' conversion used for the Coulomb square trace in~\eqref{eq:trace_square_unified}, the exchange contribution can be written compactly as
\[
e_{2,K}=\mathrm{Tr}(ZWZV).
\]
W By cyclicity of the trace, we will also use the equivalent form
\begin{equation}
\mathrm{Tr}(Z W Z V)=\mathrm{Tr}(W Z V Z).
\label{eq:exchange_trace_cyclic}
\end{equation}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.75\textwidth]{figures/12.png}
\caption{Index pairing for the exchange term and the induced cross-transpose map between $W$ and $V$.}
\label{fig:crosstranspose_exchange}
\end{figure}

Assume the ERI matrix admits a decomposition
\begin{equation}
W = W_s + W_l,
\label{eq:W_split}
\end{equation}
where $W_s$ contains the screened ``local/near-field'' entries (typically $\mathcal{O}(N^2)$ nonzeros), and $W_l$ contains the complementary far-field part (e.g.\ stored in an $\mathcal{H}^2$ or related hierarchical format). We define $V_d$ and $V_l$ \emph{as the images of $W_s$ and $W_l$ under the same cross-transpose bijection}:
\begin{equation}
V_d := \mathcal{T}_\times(W_s),\qquad V_l := \mathcal{T}_\times(W_l).
\label{eq:V_split_image}
\end{equation}
Importantly, $V_d$ here is \emph{not} defined as the ``$d$-part of $V$'' by an independent distance criterion; it is precisely the cross-transpose of $W_s$. Since $\mathcal{T}_\times$ is a bijection on the index set induced by the permutation $(\mu,\nu,\lambda,\sigma)\mapsto(\mu,\sigma,\lambda,\nu)$, it preserves entry counts: $V_d$ and $W_s$ contain the same number of stored entries, and likewise $V_l$ and $W_l$.

Using~\eqref{eq:exchange_trace_cyclic} and~\eqref{eq:V_split_image}, we expand the exchange trace as
\begin{equation}
\begin{split}
\mathrm{Tr}(W Z V Z)
&= \mathrm{Tr}\bigl((W_s+W_l)\,Z\,(V_d+V_l)\,Z\bigr) \\
&= \mathrm{Tr}(W_s Z V_d Z)+\mathrm{Tr}(W_l Z V_d Z) \\
&\quad +\mathrm{Tr}(W_s Z V_l Z)+\mathrm{Tr}(W_l Z V_l Z).
\end{split}
\label{eq:exchange_trace_expand}
\end{equation}
The first term $\mathrm{Tr}(W_s Z V_d Z)$ is the most straightforward: both $W_s$ and $V_d$ are sparse with $\mathcal{O}(N^2)$ nonzeros and are linked by the cross-transpose map, hence it can be evaluated efficiently through sparse--(diagonal)--sparse contractions. The second term $\mathrm{Tr}(W_l Z V_d Z)$ remains tractable as well because $V_d$ is sparse; one may contract $V_d Z$ first and then apply the hierarchical operator $W_l$ to the resulting object, avoiding any explicit dense far-field assembly.

The third term $\mathrm{Tr}(W_s Z V_l Z)$ can also be evaluated efficiently by exploiting the elementwise correspondence between $V_l$ and $W_l$, together with cyclicity of the trace. Using $\mathrm{Tr}(ABCD)=\mathrm{Tr}(BCDA)$, we write
\begin{equation}
\mathrm{Tr}(W_s Z V_l Z)=\mathrm{Tr}(Z W_s Z\,V_l).
\label{eq:WdVl_trace_cyclic}
\end{equation}
This suggests first forming the (still sparse) matrix $A:=Z W_s Z$, which preserves the $\mathcal{O}(N^2)$ sparsity pattern inherited from $W_s$. One then uses the cross-transpose bijection to align the indices of $A$ with those of $V_l$. In practice, it is not necessary to explicitly materialize $V_l$: whenever an entry of $V_l$ is required during the accumulation, it can be obtained by querying the corresponding entry of $W_l$. In this way, $\mathrm{Tr}(W_s Z V_l Z)$ can be evaluated while keeping the far-field object in its hierarchical representation.

The last term $\mathrm{Tr}(W_l Z V_l Z)$ is the most challenging part because it couples two far-field objects. At present, we do not have a generally efficient method for evaluating this term with the same complexity guarantees as the previous ones. A plausible direction is to incorporate Schwarz screening into the cross-transposed index space. Concretely, when the pairing changes from $(\mu\nu)$ to $(\mu\sigma)$ under $\mathcal{T}_\times$, the new pair $(\mu\sigma)$ can correspond to basis functions that are geometrically far apart, leading to integrals that are screened out by Schwarz-type bounds. If one enforces a locality/screening rule after cross-transpose, then the number of retained effective entries in the far-field component $V_l$ may become small, which in turn can make an approximate evaluation of $\mathrm{Tr}(W_l Z V_l Z)$ feasible. We are currently implementing and testing these ideas, focusing first on the first three terms in~\eqref{eq:exchange_trace_expand} and on practical screening rules for controlling the remaining far-field coupling.

\section{Comparison with tensor hyper-contraction}
\label{sec:ComparisonTHC}

It is instructive to contrast the present $\mathcal{H}^2$-compressed SOS-MP2 formulation with tensor hyper-contraction (THC), particularly least-squares THC (LS-THC) in the AO basis \cite{song2017atomic}. Both approaches ultimately exploit locality, but they do so in conceptually different ways and therefore lead to different computational bottlenecks and tunable approximations. In our algorithm, the central quantity at each Laplace quadrature point is the operator application
\begin{equation}
T = W (X \otimes Y),
\qquad
e_2^\alpha = -2\,\mathrm{Tr}\!\left[(T_s^\alpha)^2 + (T_l^\alpha)^2 + 2\,T_s^\alpha T_l^\alpha\right],
\end{equation}
where $W$ is the AO ERI tensor viewed as a matrix over AO pairs, and efficiency is obtained by (i) representing $W$ as a hierarchically compressed operator, (ii) splitting it into short- and long-range parts $W_s+W_l$, and (iii) applying $W$ to element-sparse, Laplace-weighted density-like objects $X$ and $Y$ using sparse index transformations and hierarchical basis transfers. In other words, we leave the ERIs conceptually intact and accelerate the \emph{action} of the Coulomb operator on structured inputs.

THC, by contrast, replaces the four-index ERIs by an explicit low-order tensor factorization and reorganizes the MP2 contractions around an auxiliary index set. In its canonical form, THC approximates ERIs as
\begin{equation}
(pq|rs) \approx \sum_{P,Q} X^{\mathrm{THC}}_{Pp}\,X^{\mathrm{THC}}_{Pq}\; Z_{PQ}\; X^{\mathrm{THC}}_{Qr}\,X^{\mathrm{THC}}_{Qs},
\label{eq:THCfactor}
\end{equation}
so the ``difficulty'' of the four-index object is shifted into the auxiliary core matrix $Z$ and the collocation factors $X^{\mathrm{THC}}$. In LS-THC, $X^{\mathrm{THC}}$ is prescribed on a molecular grid as (weighted) orbital values, and $Z$ is determined by a least-squares fit through a metric on the auxiliary space. When implemented in the AO basis, the practical advantage is that $X^{\mathrm{THC}}$ can be made sparse due to the locality of atom-centered Gaussian functions, and one can screen small collocation values using a user threshold (often denoted $X_{\mathrm{thre}}$). This moves much of the remaining work into matrix multiplications over the auxiliary indices, which is highly compatible with GPU-accelerated linear algebra.

This difference in viewpoint leads to a different cost profile. In our $\mathcal{H}^2$ approach, the dominant operations are the repeated sparse index transformations through $W_s$ and the hierarchical long-range transformations through $W_l$, followed by trace accumulation using the split $T=T_s+T_l$. In AO-THC/LS-THC, the \emph{energy} evaluation can be organized to be low-scaling once $X^{\mathrm{THC}}$ and $Z$ are available, but the precomputation of $Z$ introduces an additional global step: forming and (pseudo-)inverting an auxiliary-space metric (and, in practice, applying eigenvalue truncation for stability). Thus, compared to our method—which controls accuracy primarily through hierarchical low-rank truncations in $W_l$ and sparsification thresholds for $X$, $Y$, and intermediates—THC controls accuracy primarily through the quality/size of the auxiliary grid and the numerical stabilization choices made in constructing $Z$.

Finally, the two methods favor different implementation styles. The present algorithm is naturally expressed as a sequence of operator applications and tree-based traversals with irregular sparsity patterns, which maps well onto CPUs and task-based parallelism and benefits from the separability of quadrature points and hierarchical blocks. THC intentionally reorganizes work into BLAS-like contractions (sparse--dense and dense--dense) over auxiliary indices, which is particularly attractive on GPUs, at the expense of additional preprocessing and storage associated with the auxiliary representation. These distinctions clarify that the two approaches are not merely notational variants: THC changes the representation of the ERIs via a fitted factorization \eqref{eq:THCfactor}, whereas our method changes the way ERIs are \emph{applied} by combining hierarchical operator compression with double sparsity in the Laplace-weighted density objects.