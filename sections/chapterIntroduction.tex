\chapter{Introduction}

Asymptotic behavior is a unifying lens through which a vast range of algorithms are conceived, analyzed, and compared. On the design side, asymptotics provides a principled target: reduce the leading-order time and memory costs by exploiting structural regularities of the data and operators rather than relying on constant-factor optimizations. On the assessment side, asymptotics supplies metrics that distinguish convergence rates and stability properties beyond implementation details, enabling scale-aware judgments about when and why one method should be preferred to another. This dissertation brings these two perspectives together in a single theme—algorithm design and algorithmic discrimination guided by asymptotic behavior—while deliberately spanning three largely independent contributions: two system-level, structure-exploiting algorithms in electronic structure theory, and a general convergence framework for multivariate iterative methods.

In large-scale scientific computing, achieving true scalability almost always hinges on uncovering and exploiting latent structure. Beyond classical sparsity, many operators and tensors in high-dimensional settings exhibit data sparsity: even though entries are not individually zero, far-field interactions admit low-rank surrogates. Hierarchical matrix techniques convert this observation into concrete data structures and algorithms. By recursively partitioning index sets and replacing admissible far-field blocks with controlled low-rank approximations, hierarchical matrices (H-/H$^2$-matrices) reduce asymptotic storage and matrix–vector multiplication from quadratic or worse to nearly linear or linearithmic, depending on rank growth and admissibility criteria. In H$^2$ variants, nested bases tie together multiple levels of the hierarchy, share subspace information across blocks, and further compress the representation in a way that directly benefits downstream solvers whose inner loops are dominated by operator applications.

These ideas are especially consequential in electronic structure theory, where the four-index electron–repulsion integral (ERI) tensor constitutes a central bottleneck. When reshaped into an $N^2 \times N^2$ matrix (with $N$ the number of one-electron basis functions), the ERI exhibits strong locality among near-field orbital pairs and low-rank structure among well-separated groups. Prior work has shown that embedding hierarchical formats into Hartree–Fock (HF) and related self-consistent field (SCF) procedures can simultaneously compress ERI storage and accelerate the construction of Fock-like intermediates, thereby reducing both memory footprint and runtime at controlled accuracy. These developments confirm a general premise that is central to this dissertation: global algorithmic gains follow from local, quantifiable structure, provided that the data layout and computational kernels are co-designed to expose and reuse that structure across scales.

Building on this background, the first part of the dissertation adapts an H$^2$-matrix representation of the ERI tensor to the spin-opposite-scaled second-order Møller–Plesset (SOS-MP2) correlation framework. The design couples three elements: a hierarchical partition that separates near- and far-field interactions, nested-basis reuse to share low-dimensional subspaces across levels, and an atomic-orbital (AO) Laplace-style factorization that turns the dominant contractions into batches of matrix–vector and small matrix–matrix operations. This pipeline reduces the theoretical work in the dominant stages from the canonical $O(N^3)$ baseline toward $O(N^2\log N)$ while achieving a corresponding decline in memory complexity due to the nested representation. Accuracy is managed through blockwise tolerances and rank caps, enabling smooth trade-offs between wall-clock time and correlation-energy error. In practice, data-locality-aware traversals, level-synchronized batching of far-field blocks, and parallel scheduling on the hierarchical frontier yield robust performance across molecular geometries and basis set sizes; the end result is a system-level method whose asymptotic profile makes problem sizes feasible that would be out of reach under unstructured dense algebra.

The second algorithmic contribution targets strongly correlated systems, where configuration-interaction (CI) methods remain a gold standard but are often memory-limited by the size of the coefficient tensor (or its matricizations). Empirically, such tensors frequently display a pronounced corner concentration under natural orderings: information density is highest near a principal corner and decays away from it. To capitalize on this anisotropy, the dissertation introduces a corner-hierarchical (CH) matrix format that aligns the recursive partition with the observed distribution of information. The associated CH-based approximated configuration interaction (CHACI) algorithm greedily refines the hierarchy according to blockwise information density and error tolerance, switching between dense storage and truncated-SVD representations as needed. By privileging the corner and its neighborhoods throughout the multilevel partition, CHACI captures more signal per degree of freedom than a single global SVD of the same unfolding. In representative active spaces and accuracy targets, this strategy yields about a 90\% reduction in storage relative to a global SVD at comparable error levels, while preserving the algebraic interfaces needed by downstream CI kernels. Theoretically, storage bounds are provided in terms of block ranks and structural parameters of corner-dominant models, and practically, the method includes tolerance-setting and error-monitoring procedures that map directly onto chemical-accuracy requirements.

Complementing these two instances of asymptotic \emph{design}, the third contribution focuses on asymptotic \emph{discrimination}: how to compare and certify the convergence of iterative algorithms with sufficient sharpness and norm independence. Classical $Q$- and $R$-orders offer valuable but sometimes coarse lenses; gaps remain when one needs norm-agnostic statements, fine-grained distinctions within broad “linear” or “superlinear” classes, or principled handling under weak smoothness (e.g., Hölder continuity) and multistep couplings. To address these needs, the dissertation develops the P-order framework, which quantifies convergence by asymptotic comparison to power-like gauges and associated classes, thereby enabling a calibrated spectrum that includes fractional-power, linearithmic, and anti-linearithmic regimes. Two operational subfamilies—QUP-order and UP-order—are introduced to make the framework immediately usable in proofs and diagnostics. Within this setting, one obtains norm-independent, precision-aware comparisons among algorithms; sharp criteria for transitions between linear and superlinear behavior under minimal differentiability; and explicit rate characterizations for $K$-point methods under $C^{K-1,\nu}$ regularity. Conceptually, P-order provides a dimensionally consistent language for convergence that mirrors how complexity theory classifies work and storage, closing several long-standing gaps in rate analysis that previously relied on stronger smoothness or problem-specific norms.

Although the three parts of the dissertation address different problems and employ distinct techniques, they are joined by a common methodology: use asymptotics to guide both how we compute and how we judge computations. The hierarchical algorithms exemplify asymptotic design: identify structure, reorganize data and kernels to reduce leading-order costs, and prove the resulting bounds; the convergence framework exemplifies asymptotic discrimination: define scale-sensitive, norm-free gauges that separate algorithms by their rates under weak assumptions. Taken together, these contributions advance scalability, reliability, and analytic clarity in high-dimensional scientific computing. On the algorithmic side, they show that compressive representations—when matched to intrinsic structure such as far-field low rank or corner concentration—can move canonical workloads from cubic toward near-quadratic–logarithmic regimes while preserving accuracy through local error control. On the analytic side, they show that convergence can be expressed in a uniform, granular vocabulary that accommodates weak smoothness, multistep couplings, and nonstandard rate families without dependence on a particular norm.

The remainder of the dissertation develops each component in detail and connects the theoretical insights with reproducible implementations. For the hierarchical SOS-MP2 pipeline, we describe the H$^2$ data structures, admissibility conditions, rank-adaptive kernels, and AO/Laplace factorization that expose the right algebraic primitives, together with end-to-end complexity and accuracy analyses and empirical scaling on representative molecular series. For the corner-hierarchical compression of FCI tensors, we define the CH format, derive storage and error bounds, and present numerical studies that demonstrate substantial memory savings and stable accuracy under increasing active-space size. For the P-order framework, we formalize the definitions, establish norm independence and comparability properties, give sharp results under weak smoothness assumptions, and apply the framework to canonical single- and multi-point schemes to illustrate how the analysis refines classical classifications. The dissertation concludes by outlining directions in which these threads can interact—for example, adaptive strategies that couple hierarchical compression with P-order diagnostics to co-optimize accuracy and complexity—and by highlighting opportunities for broader application of asymptotic design and discrimination in other areas of scientific computing.